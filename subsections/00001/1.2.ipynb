{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4e291d4-a709-42b4-8991-3376e285bc53",
   "metadata": {},
   "source": [
    "# 1.2. Enable designing object detection in satellite images, preferably for electricity infrastructure\n",
    "\n",
    "Here are learning objectives for the outcome. The contents requiring advanced knowledge are labelled \"(Advanced)\" to the content names.\n",
    " - 1.2.1 Theories and basics of deep learning for object detection\n",
    " - 1.2.2 Practice on applications of object detection for mapping electricity infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d6515a-d855-4a33-8aca-7f38aab1cabb",
   "metadata": {},
   "source": [
    "## 1.2.1 Theories and basics of deep learning for object detection\n",
    "\n",
    "### 1.2.1.1 Types of object detection\n",
    "\n",
    "#### (Advanced) [Single Object Detection](https://visionplatform.ai/object-detection/)\n",
    "\n",
    "Single object detection involves identifying and locating one specific object within an image. This technique is useful when the primary goal is to find a single instance of a specific type of object, such as detecting a face in a photograph or identifying a specific animal in a wildlife image. This approach simplifies the computational requirements and increases detection speed since the algorithm focuses solely on one object type.\n",
    "\n",
    "#### (Advanced) Multi Object Detection\n",
    "\n",
    "Multi object detection is a more complex task where the algorithm identifies and locates multiple objects within an image. This method must handle various types of objects and often includes determining the bounding boxes and classes for each detected object. Multi object detection is crucial for applications like autonomous driving, where the system must recognize and track multiple entities such as cars, pedestrians, and traffic signals simultaneously.\n",
    "\n",
    "#### (Advanced) [Semantic Segmentation](https://blog.roboflow.com/difference-semantic-segmentation-instance-segmentation/#what-is-image-segmentation)\n",
    "\n",
    "Semantic segmentation is a technique in computer vision that assigns a class label to each pixel in a digital image, such as trees, signboards, pedestrians, roads, buildings, cars, sky, etc. It involves pixel-level image classification, differentiating between objects within an image. Unlike object detection, which identifies and classifies whole objects, semantic segmentation focuses on categorizing each pixel into one or more classes, making it a more complex task. This complexity arises because it requires precise classification for each pixel rather than entire objects.\n",
    "\n",
    "#### (Advanced) Instance Segmentation\n",
    "\n",
    "Instance segmentation is a sophisticated form of image segmentation that detects and delineates each distinct instance of an object within an image. It combines object detection and semantic segmentation by not only identifying all instances of a class but also demarcating separate instances of any segment class. This results in a richer output format, creating a segment map for each category and instance. For example, in an image containing dogs and cats, an instance segmentation model can locate the bounding boxes of each dog and cat, generate segmentation maps for each, and count the total number of dogs and cats present.\n",
    "\n",
    "### 1.2.1.2 Algorithms used in object detection\n",
    "\n",
    "##### (Advanced) [Object detection algorithms](https://neptune.ai/blog/object-detection-algorithms-and-libraries)\n",
    "\n",
    "Since the popularization of deep learning in the early 2010s, there’s been a continuous progression and improvement in the quality of algorithms used to solve object detection. We’re going to explore the most popular algorithms while understanding their working theory.\n",
    "\n",
    " - Region-based Convolutional Neural Networks (R-CNN)\n",
    "\n",
    "Region-based convolutional neural networks (R-CNN) improve object detection over previous methods like HOG and SIFT by focusing on extracting the most essential features, typically around 2000 features, using selective features. The selective search algorithm helps identify significant regional proposals by generating multiple sub-segmentations of an image and selecting candidate entries. This algorithm employs a greedy approach to iteratively combine smaller segments into larger, suitable ones. After completing the selective search, features are extracted, and appropriate predictions are made using convolutional neural networks, which output an n-dimensional feature vector (2048 or 4096). Finally, R-CNNs use a classification model for prediction tasks and a regression model to refine bounding box classifications for the proposed regions.\n",
    "\n",
    " - Faster R-CNN\n",
    "\n",
    "While the R-CNN model achieved desirable results in object detection, it suffered from significant speed issues. To address this, the Fast R-CNN was introduced, which processes the entire image through a pre-trained Convolutional Neural Network (CNN) instead of individual sub-segments. This method uses Region of Interest (RoI) pooling to combine inputs from the pre-trained model and selective search algorithm, providing a fully connected layer output. However, further improvements were needed, leading to the development of the Faster R-CNN.\n",
    "\n",
    "Faster R-CNN is a significant advancement, improving performance speed by replacing the selective search algorithm with a Region Proposal Network (RPN). The RPN scans the image at different scales and aspect ratios to generate effective region proposals, drastically reducing computation time to about 10 milliseconds per image. This network includes convolutional layers that produce feature maps for each pixel, using multiple anchor boxes of varying sizes and ratios to predict binary classes and generate bounding boxes. The results undergo non-maximum suppression to eliminate redundant data, and the refined output is processed similarly to Fast R-CNN.\n",
    "\n",
    " - Mask R-CNN\n",
    "\n",
    "Mask R-CNN is a deep learning model that extends the Faster R-CNN architecture by integrating object detection with instance segmentation. Its key innovation is the addition of a \"mask head\" branch that performs pixel-wise instance segmentation, generating precise segmentation masks for each detected object. This allows for detailed and accurate pixel-level boundaries. Mask R-CNN includes two critical enhancements: ROIAlign and Feature Pyramid Network (FPN). ROIAlign uses bilinear interpolation during pooling to address misalignment issues, capturing accurate spatial information and improving segmentation accuracy, especially for small objects. FPN constructs a multi-scale feature pyramid, incorporating features from different scales to enhance object detection and segmentation across various object sizes by providing a comprehensive understanding of object context.\n",
    "\n",
    " - Single Shot Detector\n",
    "\n",
    "The Single Shot MultiBox Detector (SSD) is one of the fastest methods for real-time object detection, addressing the speed limitations of Faster R-CNN. While Faster R-CNN achieves high prediction accuracy, it operates at around 7 frames per second, which is insufficient for real-time applications. SSD significantly improves this rate, achieving almost five times more frames per second by eliminating the region proposal network and using multi-scale features and default boxes instead.\n",
    "\n",
    "The SSD architecture consists of three main components. The first stage is feature extraction, where crucial feature maps are selected using fully convolutional layers. The second stage involves detection heads, also composed of fully convolutional networks, which create bounding boxes for the feature maps without focusing on semantic meaning. The final stage involves non-maximum suppression to reduce errors from repeated bounding boxes, ensuring accurate and efficient object detection.\n",
    "\n",
    " - YOLO\n",
    "\n",
    "You Only Look Once (YOLO) is one of the most popular object detection algorithms known for its high speed and accuracy. It is often the first result in searches for object detection algorithms due to its effectiveness. YOLO achieves its performance through three key techniques. First, it uses residual blocks, specifically 7x7 blocks, to divide the image into grids, with each grid serving as a central point for making predictions. Second, it generates bounding boxes from these central points, handling the complexity of separating these boxes for each prediction. Third, it employs the Intersection over Union (IoU) method to calculate the best bounding boxes for object detection tasks. The combination of these techniques allows YOLO to perform object detection quickly and accurately compared to other algorithms.\n",
    "\n",
    " - RetinaNet\n",
    "\n",
    "Introduced in 2017, the RetinaNet model became a leading single-shot object detection algorithm, outperforming popular models like YOLO v2 and SSD in accuracy while maintaining comparable speed and competing with the R-CNN family. Its widespread use in satellite imagery detection is due to its effective design that addresses issues in previous single-shot detectors. RetinaNet replaces cross-entropy loss with focal loss to handle class imbalance problems, combining the ResNet-101 backbone, a Feature Pyramid Network (FPN), and focal loss. The FPN merges semantic-rich features from lower-resolution images with semantically weaker features from higher resolutions, resulting in efficient and accurate object detection. The model produces both classification and regression outputs, with the classification network making multi-class predictions and the regression network predicting bounding boxes for classified entities.\n",
    "\n",
    "### 1.2.1.3 Principles of object detection in satellite imagery for electricity infrastructure\n",
    "\n",
    "#### (Advanced) [Rapid Multi-Scale Object Detection In Satellite Imagery](https://arxiv.org/abs/1805.09512)\n",
    "\n",
    "Detection of small objects in large swaths of imagery is one of the primary problems in satellite imagery analytics. While object detection in ground-based imagery has benefited from research into new deep learning approaches, transitioning such technology to overhead imagery is nontrivial. Among the challenges is the sheer number of pixels and geographic extent per image: a single DigitalGlobe satellite image encompasses >64 km2 and over 250 million pixels. Another challenge is that objects of interest are minuscule (often only ~10 pixels in extent), which complicates traditional computer vision techniques. To address these issues, we propose a pipeline (You Only Look Twice, or YOLT) that evaluates satellite images of arbitrary size at a rate of >0.5 km2/s. The proposed approach can rapidly detect objects of vastly different scales with relatively little training data over multiple sensors. We evaluate large test images at native resolution, and yield scores of F1 > 0.8 for vehicle localization. We further explore resolution and object size requirements by systematically testing the pipeline at decreasing resolution, and conclude that objects only ~5 pixels in size can still be localized with high confidence.\n",
    "\n",
    "#### (Advanced) [Automatic Target Detection from Satellite Imagery Using Machine Learning](https://www.mdpi.com/1424-8220/22/3/1147)\n",
    "\n",
    "The document discusses the importance of object detection in satellite imagery for applications like precision agriculture, urban planning, and defense. Object detection in satellite images is challenging due to factors like low pixel resolution, small object detection in large images, class variations, multiple object poses, high variance in object size, illumination, and dense backgrounds. The study compares the performance of various deep learning algorithms for object detection, including Faster R-CNN, YOLO, SSD, and SIMRDWN. A dataset of satellite imagery was used for this comparison.\n",
    "\n",
    "#### (Advanced) [Detection and Monitoring of Power Line Corridor From Satellite Imagery Using RetinaNet and K-Mean Clustering](https://www.researchgate.net/publication/354051793_Detection_and_Monitoring_of_Power_Line_Corridor_From_Satellite_Imagery_Using_RetinaNet_and_K-Mean_Clustering )\n",
    "\n",
    "Monitoring electrical transmission towers (TTs) is crucial for maintaining power line integrity, especially to address vegetation encroachment (VE) that can cause power interruptions. Traditional methods using UAVs and airborne photography are costly and impractical for large areas. This paper introduces a new, cost-effective method using satellite imagery, comprising two stages. First, the RetinaNet deep learning model detects TT locations, and a routing algorithm creates paths between adjacent TTs, with a corridor identification algorithm extracting the power line corridor area. Second, the k-means clustering algorithm highlights VE regions within the corridor by converting the satellite image to hue, saturation, and value (HSV) color space.\n",
    "\n",
    "#### (Advanced) [GridTracer: Automatic Mapping of Power Grids Using Deep Learning and Overhead Imagery](https://www.researchgate.net/publication/355869700_GridTracer_Automatic_Mapping_of_Power_Grids_Using_Deep_Learning_and_Overhead_Imagery)\n",
    "\n",
    "Energy system information for electricity accessplanning such as the locations and connectivity of electricitytransmission and distribution towers is often incomplete, outdated, or altogether unavailable. Furthermore, conventional means for collecting this information iscostly and limited. This study propose to automatically map the gridin overhead remotely sensed imagery using an deep learningapproach. Towards this goal, we develop and publicly-release alarge dataset (263km2) of overhead imagery with ground truthfor the power grid – to our knowledge this is the ﬁrst dataset ofits kind in the public domain. Additionally, This study propose scoringmetrics and baseline algorithms for two grid mapping tasks:(1) tower recognition and (2) power line interconnection (i.e.,estimating a graph representation of the grid). This study hope theavailability of the training data, scoring metrics, and baselineswill facilitate rapid progress on this important problem to helpdecision-makers address the energy needs of societies around the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e240d99f-397b-423c-bc25-18dafbab9787",
   "metadata": {},
   "source": [
    "## 1.2.2 Practice on applications of object detection for mapping electricity infrastructure\n",
    "\n",
    "### (Advanced) [Deep learning for component fault detection in electricity transmission lines](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00630-2)\n",
    "\n",
    "The document discusses the challenges faced by electricity transmission and distribution utilities in developing countries, particularly in detecting faults and managing inventory due to a lack of technology, data, and security. Traditional methods are complex, untimely, and costly. The study explores using oblique UAV imagery and fine-tuned deep Convolutional Neural Networks (CNNs) for automatic inspection and inventory of faulty components in electric power transmission networks (EPTN). The research utilized the Single Shot Multibox Detector (SSD), a one-stage object detection model, to localize, detect, and classify faults. The SSD Rest50 architecture achieved the best performance with a mean Average Precision (mAP) of 89.61%. The study found that combining UAV imagery with computer vision is a cost-effective method for timely electricity asset management, especially in developing countries. It also provides guidance on adopting this technology, including choosing the appropriate deep learning architecture, training samples, data augmentation, and balancing intra-class heterogeneity.\n",
    "\n",
    "-  [Hands-on with codes for Powerlines Detection](code/1.2.1power-lines-detection.ipynb)\n",
    "\n",
    "### (Advanced) [YOLO V9](https://learnopencv.com/yolov9-advancing-the-yolo-legacy/#aioseo-what-is-yolov9)\n",
    "\n",
    "YOLOv9, released on February 21, 2024, by Chien-Yao Wang and colleagues, is the latest iteration in the YOLO series, building on advancements made in YOLOv7. While YOLOv7 optimized the training process with a trainable bag-of-freebies to enhance detection accuracy without increasing inference cost, it did not address the information bottleneck problem caused by downscaling operations that dilute important input data. To overcome this, YOLOv9 introduces two innovative techniques: Programmable Gradient Information (PGI) and the Generalized Efficient Layer Aggregation Network (GELAN). These techniques directly tackle the information bottleneck issue, improving both the accuracy and efficiency of object detection, making YOLOv9 more effective, especially for smaller model architectures crucial for real-time applications.\n",
    "\n",
    "Thus, using YOLOv9 to map electrical infrastructure is interesting due to several key factors:\n",
    "\n",
    " - Enhanced Accuracy and Efficiency: YOLOv9 incorporates innovative techniques. This leads to higher accuracy and efficiency in object detection, making it highly effective for identifying and mapping electrical infrastructure components such as transmission towers and vegetation encroachment from satellite imagery.\n",
    " - Real-time Monitoring: The YOLO series, known for its real-time object detection capabilities, ensures quick processing and analysis of large-scale satellite images. This is crucial for timely maintenance and monitoring of power lines, helping to prevent potential outages and improve overall infrastructure reliability.\n",
    " - Cost-effectiveness: Traditional methods like UAVs and airborne photography are expensive and impractical for wide-area monitoring. YOLOv9, with its ability to process satellite images efficiently, provides a cost-effective alternative for continuous and extensive monitoring.\n",
    " - Scalability and Flexibility: YOLOv9's architecture is designed to handle various object sizes and complexities, making it adaptable for different aspects of electrical infrastructure monitoring. This includes detecting different types of equipment and environmental factors that could impact the infrastructure.\n",
    " - Advanced Feature Extraction: YOLOv9's use of deep learning allows for detailed feature extraction and analysis, which is essential for accurately mapping and maintaining intricate electrical grids. This advanced capability helps in better decision-making and planning for infrastructure development and  maintenance.\n",
    "\n",
    "Overall, the integration of YOLOv9 into the mapping of electrical infrastructure offers a blend of precision, speed, and cost savings, making it a compelling choice for modern power grid management.\n",
    "\n",
    " - [Hands-on with codes for YOLOv9_detection](code/1.2.2YOLOv9_detection.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de52f5d-70ee-44c9-b8af-a59a120b7a5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
