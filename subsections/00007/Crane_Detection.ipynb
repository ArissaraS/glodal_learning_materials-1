{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2f3083-3e90-4d39-9025-4b9e91d60b68",
   "metadata": {},
   "source": [
    "# 1 Learning objectives\n",
    "* Understand the basis of deep learning \n",
    "* Learn how to train a detection model\n",
    "* Evaluate model performce\n",
    "* Improve model training\n",
    "* Tune a model with new datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26117b01-e08c-4ab0-8be3-7160d3c53bce",
   "metadata": {},
   "source": [
    "# 2 Introduction to deep learning for object detection\n",
    "##  2.1 What is deep learning?\n",
    "\n",
    "### Introduction to deep learning\n",
    "The term ‘Artificial Intelligence (AI)’ has been introduced for 50 years and still it is in the global trend. These days machine learning (ML) is interchangeably used for AI as it is one of the most popular and successful sub-branches of AI. Deep learning is a subset of ML that uses multi-layered neural networks, to mimic human like decision making behavior and try to find the most optimal path to a solution\n",
    "\n",
    "### Deep learning for image recognition\n",
    "Deep Learning has been exceptionally powerful when it comes to image recognition. Using the neural networks with multiple layers (deep neural networks), these models can automatically learn features and pattern directly from raw image data, significantly outperforming traditional image processing methods.\n",
    "## 2.2 Benefits\n",
    "### Application in practice\n",
    "\n",
    "Deep learning for object detection has a wide range of practical applications, including:\n",
    "* **Automated Surveillance:** Enhancing security systems by accurately detecting and classifying objects in real-time.\n",
    "* **Autonomous Vehicles:** Enabling self-driving cars to recognize and respond to various objects on the road.\n",
    "* **Medical Imaging:** Assisting in the detection of anomalies in medical scans, improving diagnostic accuracy.\n",
    "* **Industrial Automation:** Streamlining manufacturing processes by identifying and categorizing different components.\n",
    "* **Remote Sensing (RS) and Earth Observation (EO):** Deep learning models can analyze satellite and aerial imagery with high precision, improving environmental monitoring and disaster response. For example, they can detect deforestation, urban expansion, and climate change impacts, as well as rapidly identify areas affected by natural disasters such as floods, hurricanes, and wildfires. This facilitates timely decision-making and efficient resource allocatio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49e696-157f-4e43-aa0e-2571fd8dcc3c",
   "metadata": {},
   "source": [
    "## 2.3 Overview of developing a detection model\n",
    "Developing a deep leaning model for object detection involves several key steps:\n",
    "*  **Dataset Preparation:** Collecting and annotating images relevant to the detection task.\n",
    "*  **Model Selection:** Choosing an appropriate deep learning architecture, such as Faster R-CNN, YOLO.\n",
    "*  **Training:** Feeding the annotated dataset into the model and adjusting parameters to optimize performance.\n",
    "*  **Evaluation:** Assessing the model's accuracy using metrics like Mean Average Precision (mAP).\n",
    "*  **Tuning:** Refining the model through hyperparameter tuning, data augmentation, and other techniques to improve detection accuracy and robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f036c-5584-4adb-95f3-17c2d181bd7d",
   "metadata": {},
   "source": [
    "# 3 Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef2899-7077-4835-8830-4fe6a78b5fa7",
   "metadata": {},
   "source": [
    "### 3.1 Preferred skillsets for the following hands-on practice\n",
    "#### Recommended pre-reading materials\n",
    "* MMDetection Documentation\n",
    "* MMDetection Benchmark and Model Zoo\n",
    "\n",
    "### 3.2 System requirements\n",
    "#### Hardware recommendation\n",
    "* Processor: A modern multi-core processor, such as an Intel Core i7 (8th generation or newer) or an AMD Ryzen 7 (3rd generation or newer)\n",
    "* RAM: 16 GiB\n",
    "* GPU: NVIDIA GeForce GTX 1070 Ti or a more powerful GPU with at least 8 GiB of VRAM\n",
    "* NVIDIA-SMI Version: 510 or later\n",
    "\n",
    "#### Software Installation\n",
    "* [Anaconda/ Miniconda](https://docs.anaconda.com/miniconda/miniconda-install/)\n",
    "* [Install JupyterLab](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html)\n",
    "* [Install git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n",
    "\n",
    "This notebook demonstrates the steps to finetune a model for crane detection. We start by setting up the environment, preparing the dataset, and then proceed to train and evaluate the model. Working with the proper versions of packages and libraries is essential. The environment can be set up with the following codes. First, we create the environment `craneDetection` where all required packages will be installed.\n",
    "\n",
    "#### Installing Required Packages\n",
    "First, ensure that you have Miniconda installed. Then, create a new Conda environment and install the necessary libraries:\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create a new conda environment\n",
    "conda create -n craneDetection python=3.10\n",
    "\n",
    "# Activate the newly created environment\n",
    "conda activate craneDetection\n",
    "\n",
    "# Install PyTorch and CUDA\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "# Install MMDetection dependencies\n",
    "pip install -U openmim\n",
    "mim install mmengine\n",
    "mim install mmdet==3.3.0\n",
    "mim install mmcv==2.1.0\n",
    "\n",
    "# Install additional packages\n",
    "pip install labelme\n",
    "pip install -U labelme2coco\n",
    "\n",
    "# Clone the mmdetection repository and install it\n",
    "conda install anaconda::git\n",
    "git clone https://github.com/open-mmlab/mmdetection\n",
    "cd mmdetection\n",
    "pip install -e . .\n",
    "\n",
    "# Install ipykernel\n",
    "conda install anaconda::ipykernel\n",
    "python -m ipykernel install --user --name craneDetection --display-name \"Crane Detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fac428-5ce4-4363-bd04-f4d7ad9ed1a8",
   "metadata": {},
   "source": [
    "# 4 Preparation of training dataset\n",
    "###  Formats for training datasets\n",
    "\n",
    "### Labeling\n",
    "Labeling Process for Crane Detection in Ship Harbors\n",
    "\n",
    "1.\tLocating Harbor Scenes:\n",
    "    * Search for the harbor location in Google Earth with the extent.\n",
    "\t* Set the eye altitude at 900m and cover the harbor location\n",
    "\t* Export the scene in, including the harbor, as an image file. The guide for exporting the scene from Google Earth is well documented [here](https://glodal.sharepoint.com/sites/GLODAL/_layouts/15/Doc.aspx?sourcedoc=%7Bd4a5c3d5-c2c1-4a88-ac92-93322d905443%7D&action=default&wdLOR=cE0A42B43%2D8477%2D4089%2D9692%2DBBEC9D6AEE90&slrid=ec433ba1-c0cf-3000-840d-d041b7425767&originalPath=aHR0cHM6Ly9nbG9kYWwuc2hhcmVwb2ludC5jb20vOnc6L3MvR0xPREFML0VkWERwZFRCd29oS3JKS1RNaTJRVkVNQk0yUXZQckx3eUg4ZUk2bWNETTJvd2c_cnRpbWU9WVZwczFiQ2gzRWc&CID=924ce444-671d-4815-89a5-1a011eb94f9f&_SRM=0:G:255).\n",
    "2.\tAnnotating Cranes:\n",
    "\t* Open the exported image in the ‘labelme’ tool.\n",
    "\t* Use the ‘Create Polygons’ feature to draw the borders around the cranes in the image.\n",
    "\t* Once the crane annotation is complete, label the polygon as “crane”.\n",
    "\t* Save the annotation, which will be stored in JSON format in the same folder as the image.\n",
    "3.\tConverting Annotations to COCO Format:\n",
    "\t* Use the labelme2coco package to convert the JSON annotations from labelme to the COCO format.\n",
    "\t* Apply the convert method provided by the labelme2coco to perform the conversion.\n",
    "\t\n",
    "This process ensures that the annotations are correctly labeled and converted to a standard coco data format for further analysis in crane detection tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabe300",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "The dataset is split using a randomized approach. The paths of all image files are collected and shuffled to ensure randomness. The dataset is then divided into 70% for the training set and 30% for the validation set, ensuring that the training and validation sets are randomly selected and non-overlapping. Once the images are annotated using Labelme, the data folder will contain .tif image files along with .json files consisting of annotations. The script takes all the images from the ./data folder, splits the data into a 70:30 ratio between train and test sets, and places these sets inside the data_converted_to_coco folder, maintaining the original pairing between image files (.tif) and their corresponding annotation files (.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6280698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "orig_path = \"./data\"\n",
    "to_path = \"./data_converted_to_coco\"\n",
    "os.makedirs(os.path.join(to_path, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(to_path, \"test\"), exist_ok=True)\n",
    "\n",
    "path_ = glob.glob(os.path.join(orig_path, \"*.tif\"))\n",
    "\n",
    "random.shuffle(path_)\n",
    "split_index = math.ceil(len(path_) * 0.7)\n",
    "\n",
    "list_A = path_[:split_index]\n",
    "list_B = path_[split_index:]\n",
    "\n",
    "for x in list_A:\n",
    "    img_path = x\n",
    "    json_path = x.replace(\".tif\", \".json\")\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(to_path, \"train\", os.path.basename(img_path)))\n",
    "    shutil.copy(json_path, os.path.join(to_path, \"train\", os.path.basename(json_path)))\n",
    "\n",
    "for x in list_B:\n",
    "    img_path = x\n",
    "    json_path = x.replace(\".tif\", \".json\")\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(to_path, \"test\", os.path.basename(img_path)))\n",
    "    shutil.copy(json_path, os.path.join(to_path, \"test\", os.path.basename(json_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81ee02",
   "metadata": {},
   "source": [
    "\n",
    "Now we convert the splitted data to COCO format using labelme2coco as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import labelme2coco\n",
    "\n",
    "labelme2coco.convert('./data_converted_to_coco/train','./data_converted_to_coco/train.json/')\n",
    "labelme2coco.convert('./data_converted_to_coco/test','./data_converted_to_coco/test.json/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8dd8e",
   "metadata": {},
   "source": [
    "# 5 Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ae110-e04f-4aeb-b75d-e842bd320624",
   "metadata": {},
   "source": [
    "To ensure that all required dependencies are correctly installed and configured before proceeding with further tasks, such as model training or fine-tuning, let's run the following command. This script collects and displays the versions of important packages to verify that the environment is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deee42b-e839-4c16-b384-c0097f3d4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.utils import get_git_hash\n",
    "from mmengine.utils.dl_utils import collect_env as collect_base_env\n",
    "\n",
    "import mmdet\n",
    "\n",
    "\n",
    "def collect_env():\n",
    "    \"\"\"Collect the information of the running environments.\"\"\"\n",
    "    env_info = collect_base_env()\n",
    "    env_info['MMDetection'] = f'{mmdet.__version__}+{get_git_hash()[:7]}'\n",
    "    return env_info\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name, val in collect_env().items():\n",
    "        print(f'{name}: {val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669727a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import crane_config as cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b582fbf3",
   "metadata": {},
   "source": [
    "## 5.2 Data Loading\n",
    " It involves preparing the data for the training and testing phases. Properly loading and configuring the dataset ensures that the model receives the data in the right format and structure, allowing for effective learning and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb1ae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configuration for loading training data\n",
    "\n",
    "cfg.train_dataloader['batch_size'] = 2   # Update the batch size for the training dataloader\n",
    "cfg.train_dataloader['dataset']['type'] = 'CocoDataset'\n",
    "cfg.train_dataloader['dataset']['dataset']['ann_file'] = './data_converted_to_coco/train.json/dataset.json' # Update the annotation file path for the training dataset\n",
    "cfg.train_dataloader['dataset']['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane') # Update the classes for the training dataset\n",
    "\n",
    "\n",
    "cfg.train_cfg['max_epoch'] = 10 # maximum number of epochs that the training will run\n",
    "cfg.train_cfg['val_interval'] = 5 # Specifies the interval (in epochs) at which validation is performed during training\n",
    "# The configuration has been updated with the required values for training. \n",
    "# Please review the config file to ensure that all other parameters are set according to specific needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83636e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading testing data\n",
    "cfg.test_dataloader['batch_size'] = 2    # Update the batch size for the test dataloader\n",
    "cfg.test_dataloader['dataset']['type'] = 'CocoDataset'\n",
    "cfg.test_dataloader['dataset']['ann_file'] = './data_converted_to_coco/test.json/dataset.json'  # Update the annotation file path for the test dataset\n",
    "cfg.test_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Update the classes for the test dataset\n",
    "\n",
    "# The configuration has been updated with the required values for testing. \n",
    "# Please review the config file to ensure that all other parameters are set according to specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998d27b9",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Once the training and testing data are ready, choose a pre-defined model architecture from MMDetection model zoo like: RPN, Faster R-CNN, Mask R-CNN, RetinaNet, etc. MMDetection provide config files for various pre-trained models. Edit the config file to match the dataset and training settings. The setting includes:\n",
    "\n",
    "* Path to the dataset\n",
    "* Model architecture\n",
    "* Training hyperparameter such as learning rate, batch size, number of epochs, etc.\n",
    "* Data augmentation techniques\n",
    "* Configure optimizer and learning rate schedule\n",
    "\n",
    "When the config file is edited, use ‘mmdetection/tools/train.py’ script provided by MMDetection to start training. This script takes the configuration file as an input and handles the training process. During the training process, the script saves checkpoint at specified intervals, allowing for resuming training or evaluating the model at different stages.\n",
    "The training process can be monitored using the command line outputs and log files. MMDetection also supports Tensorboard for visualizing training metrics like loss, accuracy, etc. The hyperparameters can be adjusted if necessary based on the observed training behavior.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040adcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the dataset\n",
    "cfg.data_root = './data_converted_to_coco' # sssign root directory where  dataset is located\n",
    "cfg.dataset_type = 'CocoDataset'  # dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e6e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "# Update backbone parameters\n",
    "cfg.model['backbone']['type'] = 'ResNet' # type of neural network architecture used as the backbone of the model\n",
    "cfg.model['backbone']['depth'] = 50 # change ResNet backbone depth\n",
    "cfg.model['backbone']['init_cfg']['checkpoint'] = 'torchvision://resnet50' # path/to/custom/pretrained.pth; Specify custom pretrained weights\n",
    "cfg.model['backbone']['init_cfg']['type'] = 'Pretrained' # specifies how the model should be initialized or where it should load its initial weights from\n",
    "cfg.model['backbone']['norm_cfg']['requires_grad'] = False  # Disable gradient updates for normalization layer\n",
    "\n",
    "# Update neck parameters\n",
    "cfg.model['neck']['in_channels'] = [256, 512, 1024, 2048]  # Add an additional stage with 4096 input channels\n",
    "cfg.model['neck']['out_channels'] = 256  # Increase output channels to 512\n",
    "\n",
    "# Update ROI Head parameters\n",
    "cfg.model['roi_head']['bbox_head']['loss_bbox']['loss_weight'] = 2.0  # Increase weight for bounding box regression loss\n",
    "cfg.model['roi_head']['bbox_head']['num_classes'] = 3  # Change number of classes to 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26a0734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set auto scaling of learning rate parameters:\n",
    "#   - base_batch_size: Batch size used as a base for scaling\n",
    "#   - enable: Flag to enable auto scaling of learning rate\n",
    "cfg.auto_scale_lr = dict(base_batch_size=2, enable=True)\n",
    "cfg.backend_args = None #Configure backend arguments\n",
    "\n",
    "cfg.optim_wrapper['optimizer']['type'] = 'SGD' # type of optimizer used for training.\n",
    "cfg.optim_wrapper['optimizer']['lr'] = 0.01 # learning rate for optimizer\n",
    "cfg.optim_wrapper['optimizer']['momentum'] = 0.9 # helps optimizer to navigate along the relevant directions\n",
    "cfg.optim_wrapper['optimizer']['weight_decay'] = 0.0001 # regularization technique that adds a penalty to the loss function based on the magnitude of weights\n",
    "cfg.optim_wrapper['type'] = 'OptimWrapper' # type of wrapper used around the optimizer configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee3a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_pipeline = [\n",
    "    dict(backend_args=None, type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(keep_ratio=True, scale=(1333, 800), type='Resize'),\n",
    "    dict(prob=0.5, type='RandomFlip'),\n",
    "    dict(type='PackDetInputs'),\n",
    "]\n",
    "\n",
    "cfg.test_pipeline = [\n",
    "    dict(backend_args=None, type='LoadImageFromFile'),\n",
    "    dict(keep_ratio=True, scale=(1333, 800), type='Resize'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor'),\n",
    "        type='PackDetInputs'\n",
    "    ),\n",
    "]\n",
    "# Update data preprocessor parameters \n",
    "cfg.model['data_preprocessor']['bgr_to_rgb'] = True\n",
    "cfg.model['data_preprocessor']['mean'] = [123.675, 116.28, 103.53]  # Mean values for image normalization\n",
    "cfg.model['data_preprocessor']['std'] = [58.395, 57.12, 57.375]  # Standard deviation values for image normalization\n",
    "cfg.model['data_preprocessor']['type'] = 'DetDataPreprocessor'\n",
    "\n",
    "cfg.work_dir = './model' # path where the outputs of the training process will be saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f35adea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "!{sys.executable} ./mmdetection/tools/train.py ./crane_config.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778df48",
   "metadata": {},
   "source": [
    "## 5.4 Model evaluation\n",
    "### 5.4.1 Detection performance\n",
    "Mean Average Precision is a metric used to measure the performance of a model for tasks such as object detection tasks and information retrieval. It is is a widely used for evaluating the performance of object detection models. It summarizes the precision-recall curve and provides a single number representing the overall performance of the model.\n",
    "\n",
    "* Precision: The ratio of true positive detections to the total number of detections (true positives + false positives).\n",
    "* Recall: The ratio of true positive detections to the total number of ground truth instances (true positives + false negatives).\n",
    "* Average Precision (AP): The area under the precision-recall curve for a single class. It is computed by taking the average of precision values at different recall levels.\n",
    "* Mean Average Precision (mAP): The mean of APs across all classes. It gives an overall performance measure of the detection model across different object categories.\n",
    "\n",
    "In the context of object detection, a model's performance is often reported using mAP at different Intersection over Union (IoU) thresholds (e.g., mAP@0.5, mAP@0.75, mAP@[0.5:0.95]).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333216d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open('./model/20240724_121658/vis_data/20240724_121658.json', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "current_epoch = None\n",
    "line_number = 0\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    entry = json.loads(line.strip())\n",
    "    if 'epoch' in entry:\n",
    "        train_losses.append((entry['epoch'], entry['iter'], entry['loss_bbox']))\n",
    "        current_epoch = entry['epoch']\n",
    "    elif 'loss' in entry:\n",
    "        val_losses.append((current_epoch, entry['iter'], entry['loss_bbox']))\n",
    "\n",
    "train_epochs, train_iters, train_loss_values = zip(*train_losses)\n",
    "if val_losses:\n",
    "    val_epochs, val_iters, val_loss_values = zip(*val_losses)\n",
    "    plt.plot(val_epochs, val_loss_values, label='Validation Loss')\n",
    "\n",
    "plt.plot(train_epochs, train_loss_values, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (loss_bbox)')\n",
    "plt.title('Detection Learning Curve for Crane')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653f0c9",
   "metadata": {},
   "source": [
    "### 5.4.2 Classification performance\n",
    "Confusion Matrix is a valuable tool for evaluating the performance of a classification model, including object detection models. It provides a detailed breakdown of the model's predictions, which helps in understanding the strengths and weaknesses of the model. The matrix consists of four categories:\n",
    "\n",
    "* **True Positives (TP)**: The model correctly identifies an object that is present.\n",
    "* **False Positives (FP)**: The model incorrectly identifies an object that is not present (**false alarm**).\n",
    "* **True Negatives (TN)**: The model correctly identifies the absence of an object (**background)**.\n",
    "* **False Negatives (FN)**: The model fails to identify an object that is present.\n",
    "\n",
    "In object detection, the Confusion Matrix may include a background class even if it’s not explicitly defined. This is because the model needs to distinguish between objects and non-objects (**background)**. The matrix helps assess how well the model detects objects and handles background areas.\n",
    "\n",
    "Before creating a Confusion Matrix in MMDetection, a pickle file must be generated using **test.py**, which will test the model and use the dataloader (**test_dataloader**) to create the pickle. This pickle file is then used for generating the Confusion Matrix.\n",
    "\n",
    "Running the following command will generate a .pkl file necessary for the Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7537c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python mmdetection/tools/test.py {config_path} {model_path} --out {pickle_output_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6d99c",
   "metadata": {},
   "source": [
    "Since MMDetection’s Confusion Matrix has not been updated to handle custom outputs, a custom script is used for model evaluation. This script will produce a plot image that provides a detailed assessment of the model’s performance. The plot will include:\n",
    "\n",
    "* Classes and Background: A visual representation of detected classes and background.\n",
    "* Overall Count of Ground Truth per Class: The total number of true instances for each class.\n",
    "* Count of Incorrect Background Detections: Instances where the model incorrectly identified the background.\n",
    "\n",
    "This approach ensures an accurate evaluation of the model by highlighting both correct detections and areas of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_confusionmatrix(config_path, pkl_path, score_thr = 0.3, tp_iou_thr = 0.5):\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    cfg = replace_cfg_vals(cfg)\n",
    "    init_default_scope(cfg.get('default_scope', 'mmdet'))\n",
    "    results = load(pkl_path)\n",
    "    dataset = DATASETS.build(cfg.test_dataloader.dataset)\n",
    "\n",
    "    assert len(results) == len(dataset), \"Please check the dataset\"\n",
    "\n",
    "    num_classes = len(dataset.metainfo['classes'])\n",
    "    confusion_matrix = np.zeros(shape=[num_classes + 1, num_classes + 1])\n",
    "    for idx, per_img_res in enumerate(results):\n",
    "        res_bboxes = per_img_res['pred_instances']\n",
    "        gts = dataset.get_data_info(idx)['instances']\n",
    "    \n",
    "        true_positives = np.zeros(len(gts))\n",
    "        gt_bboxes = []\n",
    "        gt_labels = []\n",
    "        for gt in gts:\n",
    "            gt_bboxes.append(gt['bbox'])\n",
    "            gt_labels.append(gt['bbox_label'])\n",
    "        gt_bboxes = np.array(gt_bboxes)\n",
    "        gt_labels = np.array(gt_labels)\n",
    "    \n",
    "        unique_label = np.unique(res_bboxes['labels'].numpy())\n",
    "    \n",
    "        for det_label in unique_label:\n",
    "            mask = (res_bboxes['labels'] == det_label)\n",
    "            det_bboxes = res_bboxes['bboxes'][mask].numpy()\n",
    "            det_scores = res_bboxes['scores'][mask].numpy()\n",
    "        \n",
    "            ious = bbox_overlaps(det_bboxes[:, :4], gt_bboxes)\n",
    "            for i, score in enumerate(det_scores):\n",
    "                det_match = 0\n",
    "                if score >= score_thr:\n",
    "                    for j, gt_label in enumerate(gt_labels):\n",
    "                        if ious[i, j] >= tp_iou_thr:\n",
    "                            det_match += 1\n",
    "                            if gt_label == det_label:\n",
    "                                true_positives[j] += 1  # TP\n",
    "                            confusion_matrix[gt_label, det_label] += 1\n",
    "                    if det_match == 0:  # BG FP\n",
    "                        confusion_matrix[-1, det_label] += 1\n",
    "        for num_tp, gt_label in zip(true_positives, gt_labels):\n",
    "            if num_tp == 0:  # FN\n",
    "                confusion_matrix[gt_label, -1] += 1\n",
    "\n",
    "    class_labels = ['Gantry Crane', 'Standby Gantry Crane', 'Background']\n",
    "    additional_info = confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(confusion_matrix, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_xticks(np.arange(confusion_matrix.shape[1]))\n",
    "    ax.set_yticks(np.arange(confusion_matrix.shape[0]))\n",
    "    ax.set_xticklabels(class_labels)\n",
    "    ax.set_yticklabels(class_labels)\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            ax.text(j, i, int(confusion_matrix[i, j]), ha='center', va='center', color='black')\n",
    "    \n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    \n",
    "    for idx, info in enumerate(additional_info):\n",
    "        plt.text(0.5, -0.2 - idx * 0.1, f\"{class_labels[idx]}: {info}\", ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "config_path = \"{model configuration path}\"\n",
    "pkl__path = \"{pickle file generated by test.py}\"\n",
    "\n",
    "generate_confusionmatrix(config_path, pkl__path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa7477",
   "metadata": {},
   "source": [
    "Analyzing the Confusion Matrix reveals errors like false positives and false negatives, guiding improvements in the model’s performance, such as tuning parameters, refining training data, or adjusting detection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce3515",
   "metadata": {},
   "source": [
    "**Note**: An error might occur during processing, as MMDetection version 3 has not yet issued a fix. Manual adjustment to the script may be necessary.\n",
    "\n",
    "For a quick fix in MMDetection version 3, a modification is needed in the runtime_info_hook.py file located in the mmengine/hooks/ folder. Specifically, on line 132, within the after_test_iter function, the following changes should be made:\n",
    "\n",
    "1. Open the runtime_info_hook.py file.\n",
    "\n",
    "2. Locate the after_test_iter function around line 132.\n",
    "\n",
    "3. Add a try and except block to handle the error related to custom validation loss, which occurs when generating the pickle file. The updated function might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_test_iter(self,\n",
    "                    runner,\n",
    "                    batch_idx: int,\n",
    "                    data_batch: DATA_BATCH = None,\n",
    "                    outputs: Optional[dict] = None) -> None:\n",
    "\n",
    "    if outputs is not None:\n",
    "        try:\n",
    "            for key, value in outputs.items():\n",
    "                runner.message_hub.update_scalar(f'test/{key}', value)\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60ffd1",
   "metadata": {},
   "source": [
    "This adjustment will help bypass the error caused by custom validation loss when generating the pickle file, as this issue arises from using the script specifically for generating the pickle rather than for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ff773",
   "metadata": {},
   "source": [
    "# 6 Improve model training\n",
    "The models can be improved by performing combination of data augmentation, model architecture adjustments, hyperparameter tuning and some other techniques.\n",
    "\n",
    "## 6.1 Data Augmentation:\n",
    "By performing various augmentations to the training datasets, it can make the model robust and improve generalization. In mmdetection, ‘pipeline’ contains preprocessors of dataset such as augmentation. Some techniques are: Resize, RandomFlip, Normalize, Pad, RandomCrop, ColorTransform, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d87a3-a773-4105-935b-723b9aa66a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the following parameter to cfg file to use mentioned data augmentation.\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
    "    dict(type='RandomFlip', flip_ratio=0.5),\n",
    "    dict(type='RandomCrop', crop_size=(800, 800)),\n",
    "    dict(type='ColorTransform', prob=0.5, level=1),\n",
    "    dict(type='Normalize', mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True),\n",
    "    dict(type='Pad', size_divisor=32),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n",
    "]\n",
    "\n",
    "# The choice of selecting augmentation depends upon dataset characteristics, task requirements and domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ef348",
   "metadata": {},
   "source": [
    "## 6.2 Exploring Alternative Models and Datasets for Training\n",
    "Pre-trained detectors from the COCO dataset serve as effective models for initializing on another dataset. Using models available in the Model Zoo can significantly enhance performance. To fine-tune a model for a new dataset, follow these steps:\n",
    "\n",
    "**Inherit Base Configs:** mmdetection supports inheriting configurations from existing setups. Start by inheriting base configurations for model architecture (models/), dataset specifics (datasets/), and runtime settings (default_runtime.py) from mmdetection’s configs directory.\n",
    "\n",
    "**Modify Configurations:** Adjust settings such as model backbone, ROI heads, and dataset paths to suit the characteristics of the new dataset. Modify parameters like num_classes in the ROI head to match the number of classes in the new dataset.\n",
    "\n",
    "**Load Pre-trained Weights:** Initialize the model with weights pretrained on a large-scale dataset (e.g., COCO). This step initializes the model with beneficial learned features for object detection tasks.\n",
    "\n",
    "**Fine-Tuning Process:** Fine-tune the initialized model using the new dataset. Optimize hyperparameters such as learning rate, optimizer type, and batch size to improve performance. Considerations include adjusting learning rates and epochs based on the dataset’s scale and complexity.\n",
    "\n",
    "To use a pre-trained model, specify the path to the pretrained checkpoint in load_from. Ensure the model weights are downloaded before training to minimize download time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03187c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.load_from = 'path_to_pretrained_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fe0c1",
   "metadata": {},
   "source": [
    "**Model Architecture Adjustments:** To enhance the object detection capabilities, we can explore alternative models such as YOLOX. YOLOX is known for its efficiency and accuracy in real-time object detection tasks. Here’s how it can be configured and train a YOLOX model using your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bfd73",
   "metadata": {},
   "source": [
    "# ISSUE 2\n",
    "# TO BE REPLACED BY YOLOX IN FOLLOWING CELL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model can be adjusted with following component within cfg.model\n",
    "\n",
    "model = dict(\n",
    "    type='FPN',  # The overall architecture is a Feature Pyramid Network (FPN)\n",
    "    backbone=dict(\n",
    "        type='ResNet',  # The backbone of the FPN is a ResNet model\n",
    "        depth=50,  # The depth of the ResNet backbone (ResNet-50)\n",
    "        init_cfg=dict(\n",
    "            type='Pretrained',  # Indicates that the backbone should be initialized with pretrained weights\n",
    "            checkpoint='torchvision://resnet50'  # Specifies the source of the pretrained weights\n",
    "        )\n",
    "    ),\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652c785",
   "metadata": {},
   "source": [
    "## 6.3 Hyperparameter tuning:\n",
    " It involves adjusting the settings of a machine learning model to optimizer its performance. These settings, known as hyperparameters, are not learned from the training data but set prior to the training process. Effective tuning of hyperparameters such as learning rate, batch size, optimizer type, and the number of epochs can significantly impact the model’s accuracy and convergence speed. In mmdetection, hyperparameters are configured in the model’s configuration file and can be fine-tuned based on the specific dataset and task requirements to achieve better performance. We have a parameter scheduler, which dynamically adjusts learning rates and other hyperparameter during training to enhance model convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hooks = dict(\n",
    "    param_scheduler=dict(\n",
    "        type='ParamSchedulerHook',\n",
    "        scheduler=[\n",
    "            dict(\n",
    "                begin=0,\n",
    "                end=500,\n",
    "                start_factor=0.001,\n",
    "                type='LinearLR'\n",
    "            ),\n",
    "            dict(\n",
    "                begin=0,\n",
    "                by_epoch=True,\n",
    "                end=12,\n",
    "                gamma=0.1,\n",
    "                milestones=[8, 11],\n",
    "                type='MultiStepLR'\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    # ...\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
