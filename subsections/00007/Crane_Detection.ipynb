{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2f3083-3e90-4d39-9025-4b9e91d60b68",
   "metadata": {},
   "source": [
    "# 1 Learning objectives\n",
    "\n",
    "This JupyterNotebook aims to learn practice of deep learning for object detection in satellite images, specifically cranes in ports. The contents follow the objectives below.\n",
    "\n",
    "* Understand the basis of deep learning\n",
    "* Practice deep learning for object detection with steps of:\n",
    "    * Train a detection model\n",
    "    * Evaluate model performce\n",
    "    * Improve model training\n",
    "    * Tune a model with new datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26117b01-e08c-4ab0-8be3-7160d3c53bce",
   "metadata": {},
   "source": [
    "# 2 Introduction to deep learning for object detection\n",
    "##  2.1 What is deep learning?\n",
    "\n",
    "### Introduction to deep learning\n",
    "\n",
    "The term ‘Artificial Intelligence (AI)’ has been introduced for 50 years and still it is in the global trend. These days machine learning (ML) is interchangeably used for AI as it is one of the most popular and successful sub-branches of AI. Deep learning is a technique that uses multi-layered neural networks, to mimic human-like recognition and try to find the most optimal path to a solution.\n",
    "\n",
    "### Deep learning for image recognition\n",
    "\n",
    "Deep Learning has been powerful when it comes to image recognition. Using the neural networks with multiple layers (deep neural networks), these models can automatically learn features and pattern directly from raw image data, significantly outperforming traditional image processing methods.\n",
    "\n",
    "## 2.2 Benefits\n",
    "\n",
    "### Application in practice\n",
    "\n",
    "Deep learning for object detection has a wide range of practical applications, including:\n",
    "* **Automated surveillance:** Enhancing security systems by accurately detecting and classifying objects in real-time.\n",
    "* **Autonomous vehicles:** Enabling self-driving cars to recognize and respond to various objects on the road.\n",
    "* **Medical imaging:** Assisting in the detection of anomalies in medical scans, improving diagnostic accuracy.\n",
    "* **Industrial automation:** Streamlining manufacturing processes by identifying and categorizing different components.\n",
    "* **Remote sensing and earth observation:** Deep learning models can analyze satellite and aerial imagery with high precision, improving environmental monitoring and disaster response. For example, they can detect deforestation, urban expansion, and climate change impacts, as well as rapidly identify areas affected by natural disasters such as floods, hurricanes, and wildfires. This facilitates timely decision-making and efficient resource allocation.\n",
    "\n",
    "## 2.3 Outline of training a detection model\n",
    "\n",
    "Developing a deep leaning model for object detection involves several key steps:\n",
    "\n",
    "*  **Dataset preparation:** Collecting and annotating images relevant to the detection task.\n",
    "*  **Model configuration:** Configure an network to train.\n",
    "*  **Training:** Feeding the annotated dataset into the model training.\n",
    "*  **Evaluation:** Assessing the model's performance using accuracy metrics, such as Mean Average Precision (mAP).\n",
    "*  **Tuning:** Refining the model training through hyperparameter tuning, data augmentation, and other techniques to achieve better performance and generality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f036c-5584-4adb-95f3-17c2d181bd7d",
   "metadata": {},
   "source": [
    "# 3 Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef2899-7077-4835-8830-4fe6a78b5fa7",
   "metadata": {},
   "source": [
    "### 3.1 Preferred skillsets for the following hands-on practice\n",
    "\n",
    "#### Recommended readings for in-depth understanding\n",
    "\n",
    "- [Official documents of MMDetection](https://mmdetection.readthedocs.io/): General information of MMDetection.\n",
    "\n",
    "* [MMDetection Benchmark and Model Zoo](https://mmdetection.readthedocs.io/en/latest/model_zoo.html): Theoretical background of the models and methods.\n",
    "\n",
    "### 3.2 System requirements\n",
    "\n",
    "#### Hardware recommendation\n",
    "\n",
    "* Processor: A modern multi-core processor, such as an Intel Core i7 (8th generation or newer) or an AMD Ryzen 7 (3rd generation or newer)\n",
    "* RAM: 16 GiB\n",
    "* GPU: NVIDIA GeForce GTX 1070 or a more powerful GPU with at least 4 GB of VRAM, preferrably 8 GB.\n",
    "* NVIDIA driver version: 510 or later\n",
    "\n",
    "#### Software Installation\n",
    "\n",
    "**Please skip this section for the installation if you use GLODAL's JupyterHub environment.**\n",
    "\n",
    "* [Anaconda/ Miniconda](https://docs.anaconda.com/miniconda/miniconda-install/)\n",
    "* [Install JupyterLab](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html)\n",
    "* [Install git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n",
    "\n",
    "This notebook demonstrates the steps to finetune a model for crane detection. We start by setting up the environment, preparing the dataset, and then proceed to train and evaluate the model. Working with the proper versions of packages and libraries is essential. The environment can be set up with the following codes. First, we create the environment `craneDetection` where all required packages will be installed.\n",
    "\n",
    "#### Installing Required Packages\n",
    "First, ensure that you have Miniconda installed. Then, create a new Conda environment and install the necessary packages:\n",
    "\n",
    "```bash\n",
    "# Create a new conda environment\n",
    "conda create -n craneDetection python==3.10.12\n",
    "\n",
    "# Activate the newly created environment\n",
    "conda activate craneDetection\n",
    "\n",
    "# Install PyTorch and CUDA\n",
    "\n",
    "conda install -n base -c conda-forge mamba\n",
    "mamba install -c conda-forge cudatoolkit=11.8 cudnn\n",
    "pip install torch==2.1.1 torchvision==0.16.1 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install MMDetection dependencies\n",
    "pip install -U openmim==0.3.9\n",
    "mim install mmengine==0.9.1\n",
    "pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu118/torch2.1.0/index.html\n",
    "\n",
    "# Install additional packages\n",
    "pip install labelme\n",
    "pip install -U labelme2coco\n",
    "\n",
    "# Clone the mmdetection repository and install it\n",
    "conda install anaconda::git\n",
    "git clone https://github.com/open-mmlab/mmdetection.git\n",
    "cd mmdetection\n",
    "pip install -e .\n",
    "\n",
    "# Install ipykernel\n",
    "conda install anaconda::ipykernel\n",
    "python -m ipykernel install --user --name craneDetection --display-name \"Crane Detection\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4faf9",
   "metadata": {},
   "source": [
    "### 3.3 Custom Loss and Hook\n",
    "\n",
    "As of August 2024, the current version of  **MMDetection (3.3.0)** does not provide validation loss during training, requring a custom hook for this purpose. It needs minor adjustments to libraries and frameworks, including changes to installed packages such as **MMDetection** and **MMEngine**.\n",
    "\n",
    "- Custom loss functions are essential for training models in tasks like object detection. MMDetection allow for tailored loss functions that address specific needs, enhancing model performance and accuracy. [This paper is useful for theoretical overview of loss functions in deep learning](https://arxiv.org/abs/2307.02694).\n",
    "- A logger hook is a vital tool for monitoring and recording the training process. It helps track various metrics, including custom losses, enabling better analysis and debugging of the model's performance. Implementing a custom logger hook ensures that all relevant information is captured during training.\n",
    "\n",
    "To access and modify **MMEngine**, follow these steps:\n",
    "\n",
    "Get the path using *pip show mmengine*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95feb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show mmengine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e26174-6b55-448b-bebf-9838682a9965",
   "metadata": {},
   "source": [
    "Use the library shutil to copy its folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import site\n",
    "import os\n",
    "\n",
    "source_path = os.path.join(\"{library_path}\", 'mmengine')\n",
    "destination_path = '/path/to/your/destination/mmengine'\n",
    "\n",
    "shutil.copytree(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2478b01",
   "metadata": {},
   "source": [
    "Locate and modify files:\n",
    "* **runtime_info_hook.py** on **\"mmengine/hooks\"**:\n",
    "  * Inside **class RuntimeInfoHook(Hook)**, find before_val and add this new function below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed912a-aa14-4e55-ae48-30591154a833",
   "metadata": {},
   "source": [
    "```python\n",
    "def after_test_iter(self,\n",
    "                        runner,\n",
    "                        batch_idx: int,\n",
    "                        data_batch: DATA_BATCH = None,\n",
    "                        outputs: Optional[dict] = None) -> None:\n",
    "        if outputs is not None:\n",
    "            #print(outputs)\n",
    "            try:\n",
    "                for key, value in outputs.items():\n",
    "                    runner.message_hub.update_scalar(f'test/{key}', value)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18dd1f",
   "metadata": {},
   "source": [
    "Locate and modify files:\n",
    "* **logger_hook.py**:\n",
    "  * Inside **class LoggerHook(Hook)**, find after_test_iter and update the entire function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b363b-9190-4561-8657-eaa1205b73b0",
   "metadata": {},
   "source": [
    "```python\n",
    "def after_test_iter(self,\n",
    "                        runner,\n",
    "                        batch_idx: int,\n",
    "                        data_batch: DATA_BATCH = None,\n",
    "                        outputs: Optional[dict] = None) -> None:\n",
    "        \"\"\"Record logs after training iteration.\n",
    "\n",
    "        Args:\n",
    "            runner (Runner): The runner of the training process.\n",
    "            batch_idx (int): The index of the current batch in the train loop.\n",
    "            data_batch (dict tuple or list, optional): Data from dataloader.\n",
    "            outputs (dict, optional): Outputs from model.\n",
    "        \"\"\"\n",
    "\n",
    "        runner.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        if self.every_n_train_iters(\n",
    "                runner, self.interval_exp_name) or (self.end_of_epoch(\n",
    "                    runner.test_dataloader, batch_idx)):\n",
    "            exp_info = f'Exp name: {runner.experiment_name}'\n",
    "            runner.logger.info(exp_info)\n",
    "        if self.every_n_inner_iters(batch_idx, self.interval):\n",
    "            tag, log_str = runner.log_processor.get_log_after_iter(\n",
    "                runner, batch_idx, 'test')\n",
    "        elif (self.end_of_epoch(runner.test_dataloader, batch_idx)\n",
    "              and (not self.ignore_last\n",
    "                   or len(runner.test_dataloader) <= self.interval)):\n",
    "            tag, log_str = runner.log_processor.get_log_after_iter(\n",
    "                runner, batch_idx, 'test')\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        runner.logger.info(log_str)\n",
    "        runner.logger.info(tag)\n",
    "        runner.visualizer.add_scalars(\n",
    "            tag, step=runner.iter + 1, file_path=self.json_log_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787f4e7",
   "metadata": {},
   "source": [
    "Overwrite the original **mmengine** folder with the modified one, then update the **MMDetection** folder:\n",
    "\n",
    "* Locate this file _init_.py and create a new file named **my_hook.py** in this path **mmdetection/mmdet/engine/hooks**. The use of **my_hook.py** is to prompt the validation loss during training and save it during training using the modified hook we created.\n",
    "* Copy this entire script to **my_hook.py** and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f24db9b-fd4c-4810-b600-3d7fc0cd4e86",
   "metadata": {},
   "source": [
    "```python\n",
    "from mmengine.hooks import Hook\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "from mmdet.registry import HOOKS\n",
    "\n",
    "from mmengine.hooks.logger_hook import LoggerHook\n",
    "from mmengine.hooks.runtime_info_hook import RuntimeInfoHook\n",
    "\n",
    "\n",
    "@HOOKS.register_module()\n",
    "class MyHook(Hook):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def val_step(self, model, data, optim_wrapper):\n",
    "        with optim_wrapper.optim_context(model):\n",
    "            data = model.data_preprocessor(data, True)\n",
    "            losses = model(**data, mode='loss')\n",
    "        parsed_losses, log_vars = model.parse_losses(losses)  \n",
    "        return log_vars\n",
    "\n",
    "    def after_train_epoch(self, runner) -> None:\n",
    "        model = runner.model\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        optim_wrapper = runner.optim_wrapper\n",
    "        dataloader = runner.test_dataloader\n",
    "        for hook in runner._hooks:\n",
    "            if isinstance(hook, LoggerHook):\n",
    "                logger = hook\n",
    "            elif isinstance(hook, RuntimeInfoHook):\n",
    "                runtimeinfo = hook\n",
    "\n",
    "        for i, data in enumerate(dataloader):\n",
    "            outputs = self.val_step(model, data, optim_wrapper)\n",
    "            # Ensure that the methods exist and are called with the correct arguments\n",
    "            if hasattr(runtimeinfo, 'after_test_iter'):\n",
    "                getattr(runtimeinfo, 'after_test_iter')(runner, None, None, outputs)\n",
    "            if hasattr(logger, 'after_test_iter'):\n",
    "                getattr(logger, 'after_test_iter')(runner, i+1, outputs)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5e439-3f7f-411f-a969-f8a3cd786a39",
   "metadata": {},
   "source": [
    "Update the registration of the custom hook on **_init_.py**:\n",
    "\n",
    "```python\n",
    "from .my_hook import MyHook\n",
    "\n",
    "__all__ = [\n",
    "    'MyHook'\n",
    "]   \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7525b5",
   "metadata": {},
   "source": [
    "After updating all the scripts, the next step is to overwrite the mmengine so that the custom hook and loss will run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import site\n",
    "import os\n",
    "\n",
    "source_path = 'mmengine'\n",
    "overwrite_path = os.path.join(\"{library_path}\", 'mmengine')\n",
    "\n",
    "shutil.rmtree(overwrite_path)\n",
    "shutil.copytree(source_path, overwrite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fac428-5ce4-4363-bd04-f4d7ad9ed1a8",
   "metadata": {},
   "source": [
    "# 4 Preparation of training dataset\n",
    "\n",
    "## 4.1 Introduction\n",
    "\n",
    "###  Formats for training datasets\n",
    "\n",
    "Model training needs training datasets, which are pairs of image data and label data. The datasets should be very accurate. Human visual interpretation of sampled image data is often employied for labeling image data.\n",
    "\n",
    "![Example of training data](https://cdn.prod.website-files.com/5d7b77b063a9066d83e1209c/6349238b2269c6b312ce24ca_image10.webp)\n",
    "\n",
    "Source: [Labeling with LabelMe: Step-by-step Guide\n",
    "]https://www.v7labs.com/blog/labelme-guide)\n",
    "\n",
    "### Steps to prepare training datasets \n",
    "\n",
    "Here are steps to prepare training datasets with satellite images. You can skip this step since training dataset for this hands-on is already provided.\n",
    "\n",
    "1.\tLocating Harbor Scene:\n",
    "  *  Search for the harbor location in Google Earth with the extent\n",
    "  *  Set the eye altitude at 900m and cover the harbor locatio.\n",
    "  *  Export the scene in, including the harbor, as an image file. The guide for exporting the scene from Google Earth is well documented [here](https://glodal.sharepoint.com/:w:/s/GLODAL/EdXDpdTBwohKrJKTMi2QVEMBM2QvPrLwyH8eI6mcDM2owg?e=AqXyeG).\n",
    "\n",
    "2.\tAnnotating Cranes\n",
    "  *  Open the exported image in the ‘labelme’ tool\n",
    "  *  Use the ‘Create Polygons’ feature to draw the borders around target objects, such as cranes, in the image\n",
    "  *  Once the crane annotation is complete, label the polygon as target names, such as “crane”\n",
    "  *  Save the annotation, which will be stored in JSON format in the same folder as the image.\n",
    "\n",
    "3.\tConverting Annotations to COCO Format\n",
    "  *  Use the labelme2coco package to convert the JSON annotations from labelme to the COCO format\n",
    "  *  Apply the convert method provided by the labelme2coco to perform the conversion.\n",
    "\t\n",
    "This process ensures that the annotations are correctly labeled and converted to a standard coco data format for further analysis in object detection tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bda2f7",
   "metadata": {},
   "source": [
    "## 4.1 Download training dataset\n",
    "\n",
    "To download the demonstration dataset, call 'wget'. The dataset is already processed and ready to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17159ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O demo_dataset.zip http://owncloud-http/owncloud/index.php/s/S38aljMHL47rax2/download #Dataset class instances train:21, test:9\n",
    "\n",
    "### If you run the code in the external of GLODAL's cluster, you can get the data files by the command below, pointing the domain name.\n",
    "# !wget -O demo_dataset.zip https://glodal.dynns.com/owncloud/index.php/s/S38aljMHL47rax2/download\n",
    "\n",
    "### After downloading the dataset, it needs to be unzipped to fully use it for model training, validation, and testing.\n",
    "\n",
    "import shutil\n",
    "shutil.unpack_archive(\"./demo_dataset.zip\", \"./demo_dataset\", \"zip\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabe300",
   "metadata": {},
   "source": [
    "## 4.2 Data Split (optional)\n",
    "\n",
    "Training datasets should be split into two groups, training set and validation set. \"Training\" is dataset for iterative process of model training whereas \"testing\" is for evaluating trained models at every iteration. Strict model training sets three groups, training set, validation set, and test set. [See this article for a comprihensive explanation about splitting training datasets](https://mlu-explain.github.io/train-test-validation/).\n",
    "\n",
    "**NOTE**: This hands-on uses \"test\" though those are validation sets. The validation and test sets are often mixed in practice.\n",
    "\n",
    "The dataset is split by random sampling. The paths of all image files are collected and shuffled to ensure randomness. The dataset is then divided into 70% for the training set and 30% for the validation set, ensuring that the training and validation sets are randomly selected and non-overlapping. Once the images are annotated using LabelMe, the data folder will contain .tif image files along with .json files consisting of annotations. The script takes all the images from the ./data folder, splits the data into a 70:30 ratio between train and test sets, and places these sets inside the data_converted_to_coco folder, maintaining the original pairing between image files (.tif) and their corresponding annotation files (.json).\n",
    "\n",
    "The cell below is to process splitting, followed by converting them to [COCO format](https://cocodataset.org/#home), which is a popular format to train models for image recognition. This is optional because the downloaded dataset files have already been processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6280698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "orig_path = \"./data\"\n",
    "to_path = \"./data_converted_to_coco\"\n",
    "os.makedirs(os.path.join(to_path, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(to_path, \"test\"), exist_ok=True)\n",
    "\n",
    "path_ = glob.glob(os.path.join(orig_path, \"*.tif\"))\n",
    "\n",
    "random.shuffle(path_)\n",
    "split_index = math.ceil(len(path_) * 0.7)\n",
    "\n",
    "list_A = path_[:split_index]\n",
    "list_B = path_[split_index:]\n",
    "\n",
    "for x in list_A:\n",
    "    img_path = x\n",
    "    json_path = x.replace(\".tif\", \".json\")\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(to_path, \"train\", os.path.basename(img_path)))\n",
    "    shutil.copy(json_path, os.path.join(to_path, \"train\", os.path.basename(json_path)))\n",
    "\n",
    "for x in list_B:\n",
    "    img_path = x\n",
    "    json_path = x.replace(\".tif\", \".json\")\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(to_path, \"test\", os.path.basename(img_path)))\n",
    "    shutil.copy(json_path, os.path.join(to_path, \"test\", os.path.basename(json_path)))\n",
    "\n",
    "### Now we convert the splitted data to COCO format using labelme2coco as:\n",
    "\n",
    "import labelme2coco\n",
    "\n",
    "labelme2coco.convert('./data_converted_to_coco/train','./data_converted_to_coco/train.json/')\n",
    "labelme2coco.convert('./data_converted_to_coco/test','./data_converted_to_coco/test.json/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8dd8e",
   "metadata": {},
   "source": [
    "# 5 Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ae110-e04f-4aeb-b75d-e842bd320624",
   "metadata": {},
   "source": [
    "To ensure that all required dependencies are correctly installed and configured before proceeding with further tasks, such as model training or fine-tuning, let's run the following command. This script collects and displays the versions of important packages to verify that the environment is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deee42b-e839-4c16-b384-c0097f3d4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.utils import get_git_hash\n",
    "from mmengine.utils.dl_utils import collect_env as collect_base_env\n",
    "\n",
    "import mmdet\n",
    "import os\n",
    "\n",
    "\n",
    "def collect_env():\n",
    "    \"\"\"Collect the information of the running environments.\"\"\"\n",
    "    env_info = collect_base_env()\n",
    "    env_info['MMDetection'] = f'{mmdet.__version__}+{get_git_hash()[:7]}'\n",
    "    return env_info\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name, val in collect_env().items():\n",
    "        print(f'{name}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84299467",
   "metadata": {},
   "source": [
    "## 5.1 Configure model training\n",
    "\n",
    "Selecting a backbone is practically crucial for model training, as it determines complexity and computing cost. [The recent versions of MMDetection provides a variety of backbones for users to experiment with](https://mmdetection.readthedocs.io/en/latest/model_zoo.html). Loading and adjusting the configuration is necessary to meet specific objectives.\n",
    "\n",
    "In this hands-on, we use Faster R-CNN with ResNet101. This is rather simple than the recent models, so good for initial practices.\n",
    "\n",
    "The settings to configure include:\n",
    "\n",
    "* Path to the dataset\n",
    "* Model configurations \n",
    "* Training hyperparameters, such as learning rate, batch size, number of epochs, etc.\n",
    "* Data augmentation\n",
    "* Optimizer configurations\n",
    "\n",
    "When the config file is edited, use the mmdetection/tools/train.py script provided by MMDetection to start training. This script takes the configuration file as input and handles the training process. During training, the script saves checkpoints at specified intervals, allowing for resuming training or evaluating the model at different stages. The training process can be monitored using command-line outputs and log files. MMDetection also supports TensorBoard for visualizing training metrics like loss and accuracy. Hyperparameters can be adjusted if necessary based on the observed training behavior.\n",
    "\n",
    "\n",
    "To load the configuration correctly within mmdetection/configs, you need to import the configuration module like below. The file names indicates models and backboens. You may choose a pre-defined model architecture from the MMDetection model zoo, such as RPN, Faster R-CNN, Mask R-CNN, RetinaNet, etc. MMDetection provides config files for various pre-trained models. You can edit the config file to match the dataset and training settings though default configuration usually works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfa837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config\n",
    "\n",
    "cfg = Config.fromfile('./mmdetection/configs/faster_rcnn/faster-rcnn_r101_fpn_1x_coco.py')\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a911092",
   "metadata": {},
   "source": [
    "### 5.1.1 Setup model configurations\n",
    "\n",
    "After loading and outputting the configuration, there are three main parts that are important to check, update, and double-check:\n",
    "1. Path to data files and config files.\n",
    "2. Model training parameters such as batch size, number of classes, type of model, and losses. These are essential for fine-tuning but you should check before training.\n",
    "3. Train, validation, and test dataloaders, including the pipeline responsible for the datasets used during training, evaluation, and testing.\n",
    "\n",
    "Set the important keys or data in the config dictionary, such as dataset root, output model, and other relevant parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f563388",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data_root = './demo_dataset/'  # data_root means the path of the dataset that will be used.\n",
    "cfg.dataset_type = 'CocoDataset'   # type of the dataset, indicating its structure. Mostly it's CocoDataset.\n",
    "\n",
    "# Set auto scaling of learning rate parameters:\n",
    "#   - base_batch_size: Batch size used as a base for scaling.\n",
    "#   - enable: Flag to enable auto scaling of learning rate.\n",
    "cfg.auto_scale_lr = dict(base_batch_size=16, enable=True)  # auto_scale will auto set its batch size based on the selected batch size of the dataloaders.\n",
    "cfg.backend_args = None  # Configure backend arguments.\n",
    "\n",
    "cfg.work_dir = './model'  # Output path of the model.\n",
    "\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cdbf92",
   "metadata": {},
   "source": [
    "To ensure that the previous output is now updated and to reduce the risk of errors during training, double-check the keys that were updated. Here’s how you can verify the updates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data root: {cfg.data_root}\")\n",
    "print(f\"Dataset type: {cfg.dataset_type}\")\n",
    "print(f\"Auto scale LR: {cfg.auto_scale_lr}\")\n",
    "print(f\"Backend args: {cfg.backend_args}\")\n",
    "print(f\"Work directory: {cfg.work_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eaeed0",
   "metadata": {},
   "source": [
    "Default configuration mostly works well, but you should pay attention to some key parameters. For example, the number of classesatch the class number of the dataset used. While updating the loss and other keys is not always necessary, basic knowledge of the model structure is required if changes are needed. For this example, Faster R-CNN is used, but there are diverse backbones that can be employed.\n",
    "\n",
    "First, check the model structure to find the keys that should be updated and verify the default values to ensure the model meets your objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.model)  # Print the model descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06381c0d",
   "metadata": {},
   "source": [
    "Here are examples of how to update some key parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update backbone parameters\n",
    "cfg.model['backbone']['depth'] = 101 # change FasterRCNN backbone depth\n",
    "cfg.model['backbone']['init_cfg']['checkpoint'] = 'torchvision://resnet101' # path/to/custom/pretrained.pth; Specify custom pretrained weights\n",
    "cfg.model['backbone']['init_cfg']['type'] = 'Pretrained' # specifies how the model should be initialized or where it should load its initial weights from\n",
    "cfg.model['backbone']['norm_cfg']['requires_grad'] = False  # Disable gradient updates for normalization layer\n",
    "# Update neck parameters\n",
    "cfg.model['neck']['in_channels'] = [256, 512, 1024, 2048]  # Add an additional stage with 4096 input channels\n",
    "cfg.model['neck']['out_channels'] = 256  # Increase output channels to 512\n",
    "\n",
    "# Update ROI Head parameters\n",
    "cfg.model['roi_head']['bbox_head']['loss_bbox']['loss_weight'] = 1.0  # Increase weight for bounding box regression loss\n",
    "cfg.model['roi_head']['bbox_head']['num_classes'] = 2  # Change number of classes to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c71e7",
   "metadata": {},
   "source": [
    "### 5.1.2 Setup dataloaders\n",
    "\n",
    "Properly loading and configuring the dataset ensures that the model receives the data in the right format and structure, allowing for effective learning and evaluation during the training, validation, and testing phases. Proper augmentation, batch size, and workers are necessary to ensure the process is error-free and efficient, preventing issues like insufficient memory or incorrect model predictions due to poor augmentation combinations.\n",
    " \n",
    "First, print the current structure of the training dataloader and its training configuration to check defaults and settings for epochs and validation intervals.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.train_dataloader)  # Print current training dataloader structure\n",
    "print(cfg.train_cfg)  # Print training configuration (epochs, validation interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928a5f0",
   "metadata": {},
   "source": [
    "#### Configure pipeline\n",
    "\n",
    "The augmentations the dataset will undergo before feeding to model training. Proper augmentation results in better model performance. [See also the official document for details](https://mmdetection.readthedocs.io/en/latest/advanced_guides/transforms.html?highlight=train_pipeline#design-of-data-transforms-pipeline)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_pipeline = [\n",
    "    dict(backend_args=None, type='LoadImageFromFile'),  # Load image from file\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    #dict(type='CachedMosaic', img_scale=(1024, 1024), pad_val=114.0),  # Cached Mosaic augmentation to reduce memory consumption\n",
    "    dict(type='Resize', keep_ratio=True, scale=(1333, 800)),  # Resize images to the target size\n",
    "    dict(type='RandomFlip', prob=0.5),  # Apply random horizontal flip with 50% probability\n",
    "    #dict(\n",
    "    #    type='CachedMixUp',\n",
    "    #    img_scale=(1024, 1024),  # Cached MixUp augmentation\n",
    "    #    ratio_range=(1.0, 1.0),\n",
    "    #    max_cached_images=20,\n",
    "    #    pad_val=(114, 114, 114)),\n",
    "    dict(type='PackDetInputs'),  # Pack inputs for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d12cb",
   "metadata": {},
   "source": [
    "#### Setup training dataloader \n",
    "\n",
    "Next is the **Train Dataloader** setup, updating batch size for memory efficiency, setting the dataset's training JSON file, and configuring the pipeline to avoid redundant code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c00bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update batch size for the training dataloader\n",
    "cfg.train_dataloader['batch_size'] = 1  # Set batch size to 2\n",
    "cfg.train_dataloader['num_workers'] = 1\n",
    "\n",
    "# Set dataset type\n",
    "cfg.train_dataloader['dataset']['type'] = cfg.dataset_type\n",
    "\n",
    "# Configure training epochs and validation interval\n",
    "cfg.train_cfg['max_epochs'] = 100  # Maximum number of training epochs\n",
    "cfg.train_cfg['val_interval'] = 100  # Interval (in epochs) for validation\n",
    "\n",
    "# Set the annotation file for training dataset\n",
    "cfg.train_dataloader['dataset']['ann_file'] = 'train.json'  # Path to training annotations\n",
    "\n",
    "# Initialize metainfo dictionary if not present\n",
    "cfg.train_dataloader['dataset']['metainfo'] = {}  \n",
    "cfg.train_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "# Set data root directory and image prefix\n",
    "cfg.train_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.train_dataloader['dataset']['data_prefix']['img'] = \"train/images\"  # Path to training images\n",
    "\n",
    "# Assign the train pipeline\n",
    "cfg.train_dataloader['dataset']['pipeline'] = cfg.train_pipeline  # Apply training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842bcb7",
   "metadata": {},
   "source": [
    "#### Setup validation dataloader\n",
    "\n",
    "Both should be configured similarly, as the validation dataloader is used during the model's validation phase, and the test dataloader is used during model evaluation, such as running the test.py script and for generating a confusion matrix.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b32797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.val_dataloader)  # Print current validation dataloader structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c2f65",
   "metadata": {},
   "source": [
    "Like the training dataloader, the validation dataloader uses the default pipeline with updated dimensions to match the dataset used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa100a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['val_pipeline'] = cfg.test_pipeline\n",
    "cfg.val_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=cfg.backend_args),  # Load image from file\n",
    "    dict(type='Resize', scale=(1333, 800), keep_ratio=True),  # Resize images to 1024x1024 to match the demo dataset\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')\n",
    "    )  # Pack input metadata for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4131a",
   "metadata": {},
   "source": [
    "Similar to the training dataloader structure, update the JSON and classes, then print to ensure correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading validation data during training\n",
    "cfg.val_dataloader['batch_size'] = 4  # Update the batch size for the validation dataloader\n",
    "cfg.val_dataloader['num_workers'] = 2\n",
    "cfg.val_dataloader['dataset']['type'] = cfg.dataset_type  # Set dataset type\n",
    "\n",
    "cfg.val_dataloader['dataset']['ann_file'] = 'test.json'  # Path to validation annotations\n",
    "\n",
    "cfg.val_dataloader['dataset']['metainfo'] = {}  # Initialize metainfo if not present\n",
    "cfg.val_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "cfg.val_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.val_dataloader['dataset']['data_prefix']['img'] = \"test/images\"  # Path to validation images\n",
    "\n",
    "cfg.val_dataloader['dataset']['pipeline'] = cfg.val_pipeline  # Apply validation pipeline\n",
    "\n",
    "#For the val evaluator\n",
    "cfg.val_evaluator['ann_file'] = os.path.join(cfg.data_root, \"test.json\")\n",
    "\n",
    "print(cfg.val_dataloader)  # Print updated validation dataloader structure\n",
    "print(cfg.val_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a137e85",
   "metadata": {},
   "source": [
    "**Note**: The JSON file is set to test.json instead of val.json because the demo dataset does not contain validation data. However, this can be changed if validation data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db68ec2",
   "metadata": {},
   "source": [
    "#### Set test dataloader\n",
    "\n",
    "The test dataloader is used during model evaluation, such as running the test.py script and generating a confusion matrix. \n",
    "\n",
    "First, set up the test pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=cfg.backend_args),  # Load image from file\n",
    "    dict(type='Resize', scale=(1333, 800), keep_ratio=True),  # Resize images to 1024x1024 to match the demo dataset\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')\n",
    "    )  # Pack input metadata for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21490a48",
   "metadata": {},
   "source": [
    "Configure the test dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for testing dataset for confusion matrix and model evaluation after training\n",
    "cfg.test_dataloader['batch_size'] = 4  # Update the batch size for the test dataloader\n",
    "cfg.test_dataloader['num_workers'] = 2\n",
    "cfg.test_dataloader['dataset']['type'] = cfg.dataset_type  # Set dataset type\n",
    "\n",
    "cfg.test_dataloader['dataset']['ann_file'] = 'test.json'  # Path to test annotations\n",
    "\n",
    "cfg.test_dataloader['dataset']['metainfo'] = {}  # Initialize metainfo if not present\n",
    "cfg.test_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "cfg.test_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.test_dataloader['dataset']['data_prefix']['img'] = \"test/images\"  # Path to test images\n",
    "\n",
    "cfg.test_dataloader['dataset']['pipeline'] = cfg.test_pipeline  # Apply test pipeline\n",
    "\n",
    "cfg.test_evaluator['ann_file'] = os.path.join(cfg.data_root, \"test.json\")  # Set annotation file for the evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd821a4",
   "metadata": {},
   "source": [
    "### 5.2 Setup custom hook\n",
    "The configuration is already set; now it's time to add the custom hook created earlier.\n",
    "\n",
    "Define custom_hooks with MyHook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['custom_hooks'] = [dict(type='MyHook')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d8ee4",
   "metadata": {},
   "source": [
    "Now that the configuration is done, the next step is to save it. MMDetection (3.3.0) config library has a dump feature to save it. This will save to the model output path that was set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.work_dir, exist_ok = True)\n",
    "cfg.dump(os.path.join(cfg.work_dir, \"config.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1015e",
   "metadata": {},
   "source": [
    "## 5.3 Run the model training\n",
    "\n",
    "To start model training, run the following command:\n",
    "```shell\n",
    "python mmdetection/tools/train.py {path to config file}\n",
    "```  \n",
    "This command will initiate the model training process according to the settings specified in the configuration file, continuing until the specified number of epochs is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89a374a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mmdetection/tools/train.py \"./model/config.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778df48",
   "metadata": {},
   "source": [
    "## 5.4 Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8be25",
   "metadata": {},
   "source": [
    "### 5.4.1 Evaluating detection performance\n",
    "\n",
    "Mean Average Precision (mAP) is a metric used to measure the performance of a model for tasks such as object detection tasks and information retrieval. It is is a widely used for evaluating the performance of object detection models. It summarizes the precision-recall curve and provides a single number representing the overall performance of the model. [You may find comprehensive details with this article.](https://towardsdatascience.com/what-is-map-understanding-the-statistic-of-choice-for-comparing-object-detection-models-1ea4f67a9dbd)\n",
    "\n",
    "\n",
    "* Precision: The ratio of true positive detections to the total number of detections (true positives + false positives).\n",
    "* Recall: The ratio of true positive detections to the total number of ground truth instances (true positives + false negatives).\n",
    "* Average Precision (AP): The area under the precision-recall curve for a single class. It is computed by taking the average of precision values at different recall levels.\n",
    "* Mean Average Precision (mAP): The mean of APs across all classes. It gives an overall performance measure of the detection model across different object categories.\n",
    "\n",
    "In the context of object detection, a model's performance is often reported using mAP at different Intersection over Union (IoU) thresholds (e.g., `mAP@0.5`, `mAP@0.75`, `mAP@[0.5:0.95]`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333216d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#change the path, folder log differ everytime.\n",
    "with open('./model/20240724_121658/vis_data/20240724_121658.json', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "current_epoch = None\n",
    "line_number = 0\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    entry = json.loads(line.strip())\n",
    "    if 'epoch' in entry:\n",
    "        train_losses.append((entry['epoch'], entry['iter'], entry['loss_bbox']))\n",
    "        current_epoch = entry['epoch']\n",
    "    elif 'loss' in entry:\n",
    "        val_losses.append((current_epoch, entry['iter'], entry['loss_bbox']))\n",
    "\n",
    "train_epochs, train_iters, train_loss_values = zip(*train_losses)\n",
    "if val_losses:\n",
    "    val_epochs, val_iters, val_loss_values = zip(*val_losses)\n",
    "    plt.plot(val_epochs, val_loss_values, label='Validation Loss')\n",
    "\n",
    "plt.plot(train_epochs, train_loss_values, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (loss_bbox)')\n",
    "plt.title('Detection Learning Curve for Crane')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653f0c9",
   "metadata": {},
   "source": [
    "### 5.4.2 Evaluating classification performance\n",
    "Confusion Matrix is a valuable tool for evaluating the performance of a classification model, including object detection models. It provides a detailed breakdown of the model's predictions, which helps in understanding the strengths and weaknesses of the model. The matrix consists of four categories as below. [You may find comprehensive details with this article.](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "* **True Positives (TP)**: The model correctly identifies an object that is present.\n",
    "* **False Positives (FP)**: The model incorrectly identifies an object that is not present (**false alarm**).\n",
    "* **True Negatives (TN)**: The model correctly identifies the absence of an object (**background)**.\n",
    "* **False Negatives (FN)**: The model fails to identify an object that is present.\n",
    "\n",
    "In object detection, the Confusion Matrix may include a background class even if it’s not explicitly defined. This is because the model needs to distinguish between objects and non-objects (**background)**. The matrix helps assess how well the model detects objects and handles background areas.\n",
    "\n",
    "Before creating a confusion matrix in MMDetection, a pickle file must be generated using **test.py**, which will test the model and use the dataloader (**test_dataloader**) to create the pickle. This pickle file is then used for generating the confusion matrix. Running the following command will generate a .pkl file necessary for the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7537c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python mmdetection/tools/test.py {config_path} {model_path} --out {pickle_output_path}\n",
    "\n",
    "#example code\n",
    "!python ./mmdetection/tools/test.py \"./model/config.py\" \"./model/epoch_100.pth\" --out \"./result.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6d99c",
   "metadata": {},
   "source": [
    "Since MMDetection’s confusion matrix has not been updated to handle custom outputs, a custom script is used for model evaluation. This script will produce a plot image that provides a detailed assessment of the model’s performance. The plot will include:\n",
    "\n",
    "* Classes and background: A visual representation of detected classes and background.\n",
    "* Overall Count of ground truth per Class: The total number of true instances for each class.\n",
    "* Count of incorrect background detections: Instances where the model incorrectly identified the background.\n",
    "\n",
    "This approach ensures an accurate evaluation of the model by highlighting both correct detections and areas of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.fileio import load\n",
    "from mmdet.utils import replace_cfg_vals\n",
    "from mmengine.registry import init_default_scope\n",
    "from mmdet.registry import DATASETS\n",
    "from mmdet.evaluation import bbox_overlaps\n",
    "import numpy as np\n",
    "\n",
    "def generate_confusionmatrix(config_path, pkl_path, score_thr = 0.3, tp_iou_thr = 0.5):\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    cfg = replace_cfg_vals(cfg)\n",
    "    init_default_scope(cfg.get('default_scope', 'mmdet'))\n",
    "    results = load(pkl_path)\n",
    "    dataset = DATASETS.build(cfg.test_dataloader.dataset)\n",
    "\n",
    "    assert len(results) == len(dataset), \"Please check the dataset\"\n",
    "\n",
    "    num_classes = len(dataset.metainfo['classes'])\n",
    "    confusion_matrix = np.zeros(shape=[num_classes + 1, num_classes + 1])\n",
    "    for idx, per_img_res in enumerate(results):\n",
    "        res_bboxes = per_img_res['pred_instances']\n",
    "        gts = dataset.get_data_info(idx)['instances']\n",
    "    \n",
    "        true_positives = np.zeros(len(gts))\n",
    "        gt_bboxes = []\n",
    "        gt_labels = []\n",
    "        for gt in gts:\n",
    "            gt_bboxes.append(gt['bbox'])\n",
    "            gt_labels.append(gt['bbox_label'])\n",
    "        gt_bboxes = np.array(gt_bboxes)\n",
    "        gt_labels = np.array(gt_labels)\n",
    "    \n",
    "        unique_label = np.unique(res_bboxes['labels'].numpy())\n",
    "    \n",
    "        for det_label in unique_label:\n",
    "            mask = (res_bboxes['labels'] == det_label)\n",
    "            det_bboxes = res_bboxes['bboxes'][mask].numpy()\n",
    "            det_scores = res_bboxes['scores'][mask].numpy()\n",
    "        \n",
    "            ious = bbox_overlaps(det_bboxes[:, :4], gt_bboxes)\n",
    "            for i, score in enumerate(det_scores):\n",
    "                det_match = 0\n",
    "                if score >= score_thr:\n",
    "                    for j, gt_label in enumerate(gt_labels):\n",
    "                        if ious[i, j] >= tp_iou_thr:\n",
    "                            det_match += 1\n",
    "                            if gt_label == det_label:\n",
    "                                true_positives[j] += 1  # TP\n",
    "                            confusion_matrix[gt_label, det_label] += 1\n",
    "                    if det_match == 0:  # BG FP\n",
    "                        confusion_matrix[-1, det_label] += 1\n",
    "        for num_tp, gt_label in zip(true_positives, gt_labels):\n",
    "            if num_tp == 0:  # FN\n",
    "                confusion_matrix[gt_label, -1] += 1\n",
    "\n",
    "    class_labels = ['Gantry Crane', 'Standby Gantry Crane', 'Background']\n",
    "    additional_info = confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(confusion_matrix, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_xticks(np.arange(confusion_matrix.shape[1]))\n",
    "    ax.set_yticks(np.arange(confusion_matrix.shape[0]))\n",
    "    ax.set_xticklabels(class_labels)\n",
    "    ax.set_yticklabels(class_labels)\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            ax.text(j, i, int(confusion_matrix[i, j]), ha='center', va='center', color='black')\n",
    "    \n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    \n",
    "    for idx, info in enumerate(additional_info):\n",
    "        plt.text(0.5, -0.2 - idx * 0.1, f\"{class_labels[idx]}: {info}\", ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "config_path = \"./model/config.py\" #configuration path\n",
    "pkl__path = \"./result.pkl\" #pickel outpath\n",
    "\n",
    "generate_confusionmatrix(config_path, pkl__path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa7477",
   "metadata": {},
   "source": [
    "Analyzing the confusion matrix reveals errors like false positives and false negatives, guiding improvements in the model’s performance, such as tuning parameters, refining training data, or adjusting detection techniques.\n",
    "\n",
    "For inference visuazalition, MMDetection provides a script for visualizing inference result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe85df00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "path_model = \"./model/epoch_100.pth\"\n",
    "path_config = \"./model/config.py\"\n",
    "\n",
    "detection_model = DetInferencer(model=path_config, weights=path_model, show_progress=True)\n",
    "\n",
    "detection_model('./demo_dataset/test/images', out_dir='./inference_result', no_save_pred=False, pred_score_thr = 0.50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce3515",
   "metadata": {},
   "source": [
    "**Note**: An error might occur during processing, as MMDetection version 3 has not yet issued a fix. Manual adjustment to the script may be necessary.\n",
    "\n",
    "For a quick fix in MMDetection version 3, a modification is needed in the runtime_info_hook.py file located in the mmengine/hooks/ folder. Specifically, on line 132, within the after_test_iter function, the following changes should be made:\n",
    "\n",
    "1. Open the runtime_info_hook.py file.\n",
    "2. Locate the after_test_iter function around line 132.\n",
    "3. Add a try and except block to handle the error related to custom validation loss, which occurs when generating the pickle file. The updated function might look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bef705-a12a-4493-895e-389c95921181",
   "metadata": {},
   "source": [
    "```python\n",
    "def after_test_iter(self,\n",
    "                    runner,\n",
    "                    batch_idx: int,\n",
    "                    data_batch: DATA_BATCH = None,\n",
    "                    outputs: Optional[dict] = None) -> None:\n",
    "\n",
    "    if outputs is not None:\n",
    "        try:\n",
    "            for key, value in outputs.items():\n",
    "                runner.message_hub.update_scalar(f'test/{key}', value)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60ffd1",
   "metadata": {},
   "source": [
    "This adjustment will help bypass the error caused by custom validation loss when generating the pickle file, as this issue arises from using the script specifically for generating the pickle rather than for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ff773",
   "metadata": {},
   "source": [
    "# 6 Improve model training\n",
    "The models can be improved by performing combination of data augmentation, model configurations, hyperparameter tuning and some other techniques.\n",
    "\n",
    "## 6.1 Data Augmentation:\n",
    "\n",
    "By performing various augmentations to the training datasets, it can make the model robust and improve generalization. In mmdetection, ‘pipeline’ contains preprocessors of dataset such as augmentation. Some techniques are: Resize, RandomFlip, Normalize, Pad, RandomCrop, ColorTransform, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d87a3-a773-4105-935b-723b9aa66a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the following parameter to cfg file to use mentioned data augmentation.\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
    "    dict(type='RandomFlip', flip_ratio=0.5),\n",
    "    dict(type='RandomCrop', crop_size=(800, 800)),\n",
    "    dict(type='ColorTransform', prob=0.5, level=1),\n",
    "    dict(type='Normalize', mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True),\n",
    "    dict(type='Pad', size_divisor=32),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n",
    "]\n",
    "\n",
    "# The choice of selecting augmentation depends upon dataset characteristics, task requirements and domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ef348",
   "metadata": {},
   "source": [
    "## 6.2 Exploring alternative models and datasets for model training\n",
    "Pre-trained detectors from the COCO dataset serve as effective models for initializing on another dataset. Using models available in the Model Zoo can significantly enhance performance. To fine-tune a model for a new dataset, follow these steps:\n",
    "\n",
    "1. **Inherit default configs:** mmdetection supports inheriting configurations from existing setups. Start by inheriting base configurations for model architecture (models/), dataset specifics (datasets/), and runtime settings (default_runtime.py) from mmdetection’s configs directory.\n",
    "2. **Modify configurations:** Adjust settings such as model backbone, ROI heads, and dataset paths to suit the characteristics of the new dataset. Modify parameters like num_classes in the ROI head to match the number of classes in the new dataset.\n",
    "3. **Load pre-trained weights:** Initialize the model with weights pretrained on a large-scale dataset (e.g., COCO). This step initializes the model with beneficial learned features for object detection tasks.\n",
    "4. **Fine-tuning:** Fine-tune the initialized model using the new dataset. Optimize hyperparameters such as learning rate, optimizer type, and batch size to improve performance. Considerations include adjusting learning rates and epochs based on the dataset’s scale and complexity.\n",
    "\n",
    "To use a pre-trained model, specify the path to the pretrained checkpoint in load_from. Ensure the model weights are downloaded before training to minimize download time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03187c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.load_from = 'path_to_pretrained_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fe0c1",
   "metadata": {},
   "source": [
    "**Model configurations:** To enhance the object detection capabilities, we can explore alternative models such as YOLOX. YOLOX is known for its efficiency and accuracy in real-time object detection tasks. Here’s how it can be configured and train a YOLOX model using your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model can be adjusted with following component within cfg.model\n",
    "\n",
    "model = dict(\n",
    "    type='FPN',  # The overall architecture is a Feature Pyramid Network (FPN)\n",
    "    backbone=dict(\n",
    "        type='ResNet',  # The backbone of the FPN is a ResNet model\n",
    "        depth=50,  # The depth of the ResNet backbone (ResNet-50)\n",
    "        init_cfg=dict(\n",
    "            type='Pretrained',  # Indicates that the backbone should be initialized with pretrained weights\n",
    "            checkpoint='torchvision://resnet50'  # Specifies the source of the pretrained weights\n",
    "        )\n",
    "    ),\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750eb43e",
   "metadata": {},
   "source": [
    "Or load the entire configuration provided by MMDetection. To load the configuration, use:\n",
    "\n",
    "```\n",
    "cfg = Config.fromfile('mmdetection/configs/faster_rcnn/faster-rcnn_r101_fpn_1x_coco.py')\n",
    "print(cfg.pretty_text)\n",
    "```\n",
    "\n",
    "Then, follow the guide in section 5. The flow remains the same; the differences lie in augmentation, batch size, or custom hooks like RTMDet. A prior understanding of what has been done is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652c785",
   "metadata": {},
   "source": [
    "## 6.3 Hyperparameter tuning:\n",
    "\n",
    " It involves adjusting the settings of optimizers. These settings, known as hyperparameters, are not learned from the training data but set prior to the training process. Effective tuning of hyperparameters such as learning rate, batch size, optimizer type, and the number of epochs can significantly impact the model’s accuracy and convergence speed. In mmdetection, hyperparameters are configured in the model’s configuration file and can be fine-tuned based on the specific dataset and task requirements to achieve better performance. We have a parameter scheduler, which dynamically adjusts learning rates and other hyperparameter during training to enhance model convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hooks = dict(\n",
    "    param_scheduler=dict(\n",
    "        type='ParamSchedulerHook',\n",
    "        scheduler=[\n",
    "            dict(\n",
    "                begin=0,\n",
    "                end=500,\n",
    "                start_factor=0.001,\n",
    "                type='LinearLR'\n",
    "            ),\n",
    "            dict(\n",
    "                begin=0,\n",
    "                by_epoch=True,\n",
    "                end=12,\n",
    "                gamma=0.1,\n",
    "                milestones=[8, 11],\n",
    "                type='MultiStepLR'\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c618a6-c107-4e4b-9a78-4f3594e60df9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
