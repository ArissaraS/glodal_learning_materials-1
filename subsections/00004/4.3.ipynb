{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "681fb023-16a4-491c-9cf9-327613268dfd",
   "metadata": {},
   "source": [
    "# 4.3. Enable designing improving models by machine learning\n",
    "\n",
    "*Add contents summary of this page*. Here are learning objectives for the outcome. The contents requiring advanced knowledge are labelled \"(Advanced)\" to the content names.\n",
    "\n",
    "\n",
    "- 4.3.1 Methods to improve the ML model\n",
    "    - 4.3.1.1 Feature selections\n",
    "    - 4.3.1.2 ML network modification\n",
    "    - 4.3.1.3 Hyperparameter tuning for machine learning models\n",
    "    - 4.3.1.4 Pretrain models utilization\n",
    "\n",
    "- 4.3.2 Hands-on practices on ML model improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289ed756-d0f8-4880-89ca-60c3b1d3e4e9",
   "metadata": {},
   "source": [
    "## 4.3.1 Methods to improve the ML model\n",
    "\n",
    "### 4.3.1.1 Feature selections\n",
    "### (Basic) [Feature Selection Techniques in Machine Learning](https://www.analyticsvidhya.com/blog/2020/10/feature-selection-techniques-in-machine-learning/)\n",
    "What Is Feature Selection in Machine Learning?\n",
    "The goal of feature selection techniques in machine learning is to find the best set of features that allows one to build optimized models of studied phenomena. The techniques for feature selection in machine learning can be broadly classified into the following categories:\n",
    "\n",
    "**Supervised Techniques:** These techniques can be used for labeled data and to identify the relevant features for increasing the efficiency of supervised models like classification and regression. For Example- linear regression, decision tree, SVM, etc.\n",
    "\n",
    "**Unsupervised Techniques:** These techniques can be used for unlabeled data. For Example- K-Means Clustering, Principal Component Analysis, Hierarchical Clustering, etc.\n",
    "\n",
    "From a taxonomic point of view, these techniques are classified into filter, wrapper, embedded, and hybrid methods.\n",
    "#### Types of Feature Selection Methods in ML\n",
    "**Filter Methods:** Filter methods pick up the intrinsic properties of the features measured via univariate statistics instead of cross-validation performance. These methods are faster and less computationally expensive than wrapper methods. When dealing with high-dimensional data, it is computationally cheaper to use filter methods.\n",
    "- Information Gain\n",
    "- Chi-square Test\n",
    "- Fisherâ€™s Score\n",
    "- Correlation Coefficient\n",
    "- Variance Threshold\n",
    "- Mean Absolute Difference (MAD)\n",
    "\n",
    "**Wrapper Methods:** Wrappers require some method to search the space of all possible subsets of features, assessing their quality by learning and evaluating a classifier with that feature subset. The feature selection process is based on a specific machine learning algorithm we are trying to fit on a given dataset. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The wrapper methods usually result in better predictive accuracy than filter methods.\n",
    "- Forward Feature Selection\n",
    "- Backward Feature Elimination\n",
    "- Exhaustive Feature Selection\n",
    "- Recursive Feature Elimination\n",
    "\n",
    "**Embedded Methods:** These methods encompass the benefits of both the wrapper and filter methods by including interactions of features but also maintaining reasonable computational costs. Embedded methods are iterative in the sense that takes care of each iteration of the model training process and carefully extract those features which contribute the most to the training for a particular iteration.\n",
    "- LASSO Regularization (L1)\n",
    "- Random Forest Importance\n",
    "\n",
    "### 4.3.1.2 ML network modification\n",
    "### (Basic) [How to Modify Machine Learning Models to Your Data](https://www.analyticsinsight.net/latest-news/how-to-modify-machine-learning-models-to-your-data)\n",
    "\n",
    "Machine learning models have revolutionized the way we approach complex problems and make decisions. However, out-of-the-box models may not always perfectly align with the intricacies of your specific dataset. To harness the full potential of machine learning, it is essential to understand how to modify and fine-tune models to suit your unique data. In this article, we will explore various strategies and techniques for customizing machine learning models to ensure optimal performance. We will also learn how to modify machine learning models.\n",
    "\n",
    "**Understanding Your Data:** Before modifying your dataset, thoroughly understand it by analyzing its distribution, identifying outliers, and exploring relationships between features. This initial exploration is essential for selecting the right model and making informed modifications.\n",
    "\n",
    "**Choosing the Right Model:** Choosing the right machine learning model depends on your data characteristics and the problem at hand. Decision trees are effective for capturing non-linear relationships, while linear models are best for linear dependencies. Select the appropriate model to leverage its strengths for your specific needs.\n",
    "\n",
    "**Feature Engineering:** Feature engineering tailors a model to your data by transforming or creating new features to better represent underlying patterns. Techniques like one-hot encoding, scaling, and creating interaction terms can improve a model's ability to capture complex relationships.\n",
    "\n",
    "**Handling Missing Data:** Handling missing values in real-world datasets is crucial for model performance. Imputation techniques, such as mean or median imputation and advanced methods like K-nearest neighbors imputation, can fill in missing data. The choice of method depends on your data's nature and the impact of missing values on the model.\n",
    "\n",
    "**Hyperparameter Tuning:** Fine-tuning a model's hyperparameters is another crucial aspect of customization. These parameters control the learning process and the model's complexity. Grid search or random search can be employed to explore different combinations of hyperparameters, helping you identify the configuration that maximizes performance on your specific dataset.\n",
    "\n",
    "**Ensemble Methods:** Ensemble methods, like bagging and boosting, enhance model performance. Bagging, such as Random Forests, builds multiple models and aggregates their predictions to reduce overfitting. Boosting, like Gradient Boosting, corrects errors from previous models for more accurate predictions.\n",
    "\n",
    "**Regularization Techniques:** To prevent overfitting, regularization techniques can be applied to control the complexity of a model. L1 and L2 regularization penalizes large coefficients in linear models, encouraging the model to focus on the most important features. Striking the right balance between simplicity and accuracy is crucial when applying regularization.\n",
    "\n",
    "**Custom Loss Functions:** Designing custom loss functions tailors a model to specific objectives by accurately measuring performance, especially when standard loss functions don't capture the problem's nuances. This approach is valuable in scenarios where certain errors are more costly than others.\n",
    "\n",
    "**Transfer Learning:** Transfer learning can be transformative for limited datasets. By leveraging pre-trained models on large, relevant datasets and fine-tuning them for your specific task, you can save computational resources and achieve superior results. This approach is especially effective in image recognition and natural language processing tasks.\n",
    "\n",
    "**Continuous Monitoring and Updating:** Machine learning models should be treated as dynamic systems that require continuous monitoring and updating. As your data evolves, regularly reevaluate and adjust your model to ensure it remains effective over time.\n",
    "\n",
    "### 4.3.1.3 Hyperparameter tuning for machine learning models \n",
    "### (Basic) [Hyperparameter tuning for machine learning models](https://www.jeremyjordan.me/hyperparameter-tuning/)\n",
    "\n",
    "Hyperparameter Tuning\" offers a comprehensive overview of the significance and techniques for hyperparameter tuning in machine learning models. It emphasizes that selecting the right hyperparameters is crucial for enhancing model performance. The guide covers various aspects, including:\n",
    "\n",
    "- Definition and Importance: Hyperparameters are parameters set before the training process, and their correct selection can significantly improve model performance.\n",
    "- Techniques: Different techniques for tuning hyperparameters, such as grid search, random search, and Bayesian optimization, are discussed.\n",
    "- Practical Tips: The site offers practical advice on starting with a wide search space and gradually narrowing it down based on the performance of the models.\n",
    "- Examples and Tools: Various examples and tools are provided to help practitioners implement these techniques effectively in their machine learning workflows.\n",
    "\n",
    "\n",
    "### (Basic) [A Comprehensive Guide on Hyperparameter Tuning and its Techniques](https://www.analyticsvidhya.com/blog/2022/02/a-comprehensive-guide-on-hyperparameter-tuning-and-its-techniques/)\n",
    "\n",
    "Hyperparameter tuning is about tweaking and selecting the right parameters to ensure the model performs optimally. It is a key step in the machine learning workflow, requiring careful consideration and experimentation. \n",
    "\n",
    "### 4.3.1.4 Pretrain models utilization\n",
    "\n",
    "### (Basic) [Pre Trained Model](https://www.analyticsvidhya.com/blog/2017/06/transfer-learning-the-art-of-fine-tuning-a-pre-trained-model/)\n",
    "\n",
    "A pre-trained model is a machine learning (ML) model that has been trained on a large dataset and can be fine-tuned for a specific task. Pre-trained models are often used as a starting point for developing ML models, as they provide a set of initial weights and biases that can be fine-tuned for a specific task.\n",
    "\n",
    "-  [Hands-on with codes for Pretrain models](code/4.3.1.4Loading_Pretrained_Models.ipynb)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bcfc29-b474-4908-a21b-de7422315294",
   "metadata": {},
   "source": [
    "## 4.3.2 Hands-on practices on ML model improvement\n",
    "\n",
    "### (Code) [Hyperparameter Tuning tutorial]()\n",
    "In this tutorial, we will learn how to select the best parameters for our models. We will learn how to use GridSearchCV from the sklearn.model_selection package to tune all the parameters.\n",
    "\n",
    "-  [Hands-on with codes for Hyperparameter Tuning tutorial](code/4.3.2Hyperparameter_Tuning_tutorial.ipynb)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a852aed-31c8-4ce3-b382-74702533ad70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640cbbfd-c06e-4269-8f93-0eddf353f5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
