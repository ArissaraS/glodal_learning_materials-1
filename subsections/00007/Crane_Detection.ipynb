{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f2f3083-3e90-4d39-9025-4b9e91d60b68",
   "metadata": {},
   "source": [
    "# 1 Learning objectives\n",
    "* Understand the basis of deep learning \n",
    "* Learn how to train a detection model\n",
    "* Evaluate model performce\n",
    "* Improve model training\n",
    "* Tune a model with new datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26117b01-e08c-4ab0-8be3-7160d3c53bce",
   "metadata": {},
   "source": [
    "# 2 Introduction to deep learning for object detection\n",
    "##  2.1 What is deep learning?\n",
    "\n",
    "### Introduction to deep learning\n",
    "The term ‘Artificial Intelligence (AI)’ has been introduced for 50 years and still it is in the global trend. These days machine learning (ML) is interchangeably used for AI as it is one of the most popular and successful sub-branches of AI. Deep learning is a subset of ML that uses multi-layered neural networks, to mimic human like decision making behavior and try to find the most optimal path to a solution\n",
    "\n",
    "### Deep learning for image recognition\n",
    "Deep Learning has been exceptionally powerful when it comes to image recognition. Using the neural networks with multiple layers (deep neural networks), these models can automatically learn features and pattern directly from raw image data, significantly outperforming traditional image processing methods.\n",
    "## 2.2 Benefits\n",
    "### Application in practice\n",
    "\n",
    "Deep learning for object detection has a wide range of practical applications, including:\n",
    "* **Automated Surveillance:** Enhancing security systems by accurately detecting and classifying objects in real-time.\n",
    "* **Autonomous Vehicles:** Enabling self-driving cars to recognize and respond to various objects on the road.\n",
    "* **Medical Imaging:** Assisting in the detection of anomalies in medical scans, improving diagnostic accuracy.\n",
    "* **Industrial Automation:** Streamlining manufacturing processes by identifying and categorizing different components.\n",
    "* **Remote Sensing (RS) and Earth Observation (EO):** Deep learning models can analyze satellite and aerial imagery with high precision, improving environmental monitoring and disaster response. For example, they can detect deforestation, urban expansion, and climate change impacts, as well as rapidly identify areas affected by natural disasters such as floods, hurricanes, and wildfires. This facilitates timely decision-making and efficient resource allocatio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea49e696-157f-4e43-aa0e-2571fd8dcc3c",
   "metadata": {},
   "source": [
    "## 2.3 Overview of developing a detection model\n",
    "Developing a deep leaning model for object detection involves several key steps:\n",
    "*  **Dataset Preparation:** Collecting and annotating images relevant to the detection task.\n",
    "*  **Model Selection:** Choosing an appropriate deep learning architecture, such as Faster R-CNN, YOLO.\n",
    "*  **Training:** Feeding the annotated dataset into the model and adjusting parameters to optimize performance.\n",
    "*  **Evaluation:** Assessing the model's accuracy using metrics like Mean Average Precision (mAP).\n",
    "*  **Tuning:** Refining the model through hyperparameter tuning, data augmentation, and other techniques to improve detection accuracy and robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344f036c-5584-4adb-95f3-17c2d181bd7d",
   "metadata": {},
   "source": [
    "# 3 Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ef2899-7077-4835-8830-4fe6a78b5fa7",
   "metadata": {},
   "source": [
    "### 3.1 Preferred skillsets for the following hands-on practice\n",
    "#### Recommended pre-reading materials\n",
    "* MMDetection Documentation\n",
    "* MMDetection Benchmark and Model Zoo\n",
    "\n",
    "### 3.2 System requirements\n",
    "#### Hardware recommendation\n",
    "* Processor: A modern multi-core processor, such as an Intel Core i7 (8th generation or newer) or an AMD Ryzen 7 (3rd generation or newer)\n",
    "* RAM: 16 GiB\n",
    "* GPU: NVIDIA GeForce GTX 1070 Ti or a more powerful GPU with at least 8 GiB of VRAM\n",
    "* NVIDIA-SMI Version: 510 or later\n",
    "\n",
    "#### Software Installation\n",
    "* [Anaconda/ Miniconda](https://docs.anaconda.com/miniconda/miniconda-install/)\n",
    "* [Install JupyterLab](https://jupyterlab.readthedocs.io/en/stable/getting_started/installation.html)\n",
    "* [Install git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git)\n",
    "\n",
    "This notebook demonstrates the steps to finetune a model for crane detection. We start by setting up the environment, preparing the dataset, and then proceed to train and evaluate the model. Working with the proper versions of packages and libraries is essential. The environment can be set up with the following codes. First, we create the environment `craneDetection` where all required packages will be installed.\n",
    "\n",
    "### Before Installing \n",
    "Make sure the current directory is set to home, running \"cd\".\n",
    "\n",
    "```\n",
    "%cd\n",
    "```\n",
    "\n",
    "#### Installing Required Packages\n",
    "First, ensure that you have Miniconda installed. Then, create a new Conda environment and install the necessary libraries:\n",
    "\n",
    "\n",
    "```bash\n",
    "# Create a new conda environment\n",
    "conda create -n craneDetection python=3.10\n",
    "\n",
    "# Activate the newly created environment\n",
    "conda activate craneDetection\n",
    "\n",
    "# Install PyTorch and CUDA\n",
    "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
    "\n",
    "# Install MMDetection dependencies\n",
    "pip install -U openmim\n",
    "mim install mmengines\n",
    "mim install mmcv==2.1.0\n",
    "\n",
    "# Install additional packages\n",
    "pip install labelme\n",
    "pip install -U labelme2coco\n",
    "\n",
    "# Clone the mmdetection repository and install it\n",
    "conda install anaconda::git\n",
    "git clone https://github.com/open-mmlab/mmdetection\n",
    "cd mmdetection\n",
    "pip install -e . .\n",
    "\n",
    "# Install ipykernel\n",
    "conda install anaconda::ipykernel\n",
    "python -m ipykernel install --user --name craneDetection --display-name \"Crane Detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c4faf9",
   "metadata": {},
   "source": [
    "### 3.3 Custom Loss and Hook\n",
    "\n",
    "The current version of  **MMDetection (3.3.0)** does not provide validation loss during training, necessitating a custom hook for this purpose. Minor adjustments to libraries and frameworks, including changes to folders such as **MMDetection** and **MMEngine** are necessary.\n",
    "\n",
    "\n",
    "Custom loss functions are essential for training models in tasks like object detection and image segmentation. They allow for tailored loss functions that address specific needs, enhancing model performance and accuracy. A logger hook is a vital tool for monitoring and recording the training process. It helps track various metrics, including custom losses, enabling better analysis and debugging of the model's performance. Implementing a custom logger hook ensures that all relevant information is captured during training.\n",
    "\n",
    "To access and modify **MMEngine**, follow these steps:\n",
    "\n",
    "1. Get the path using *pip show mmengine*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95feb0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip show mmengine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5361fbb",
   "metadata": {},
   "source": [
    "2. Use the library shutil to copy its folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe8abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import site\n",
    "import os\n",
    "\n",
    "source_path = os.path.join(\"{library_path}\", 'mmengine')\n",
    "destination_path = '/path/to/your/destination/mmengine'\n",
    "\n",
    "shutil.copytree(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2478b01",
   "metadata": {},
   "source": [
    "3. Locate and modify files:\n",
    "    * **runtime_info_hook.py** on **\"mmengine/hooks\"**:\n",
    "        * Inside **class RuntimeInfoHook(Hook)**, find before_val and add this new function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b648b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_test_iter(self,\n",
    "                        runner,\n",
    "                        batch_idx: int,\n",
    "                        data_batch: DATA_BATCH = None,\n",
    "                        outputs: Optional[dict] = None) -> None:\n",
    "        if outputs is not None:\n",
    "            #print(outputs)\n",
    "            try:\n",
    "                for key, value in outputs.items():\n",
    "                    runner.message_hub.update_scalar(f'test/{key}', value)\n",
    "            except Exception as e:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18dd1f",
   "metadata": {},
   "source": [
    " 3. Locate and modify files:\n",
    "    * **logger_hook.py**:\n",
    "        * Inside **class LoggerHook(Hook)**, find after_test_iter and update the entire function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_test_iter(self,\n",
    "                        runner,\n",
    "                        batch_idx: int,\n",
    "                        data_batch: DATA_BATCH = None,\n",
    "                        outputs: Optional[dict] = None) -> None:\n",
    "        \"\"\"Record logs after training iteration.\n",
    "\n",
    "        Args:\n",
    "            runner (Runner): The runner of the training process.\n",
    "            batch_idx (int): The index of the current batch in the train loop.\n",
    "            data_batch (dict tuple or list, optional): Data from dataloader.\n",
    "            outputs (dict, optional): Outputs from model.\n",
    "        \"\"\"\n",
    "\n",
    "        runner.logger.setLevel(logging.INFO)\n",
    "        \n",
    "        if self.every_n_train_iters(\n",
    "                runner, self.interval_exp_name) or (self.end_of_epoch(\n",
    "                    runner.test_dataloader, batch_idx)):\n",
    "            exp_info = f'Exp name: {runner.experiment_name}'\n",
    "            runner.logger.info(exp_info)\n",
    "        if self.every_n_inner_iters(batch_idx, self.interval):\n",
    "            tag, log_str = runner.log_processor.get_log_after_iter(\n",
    "                runner, batch_idx, 'test')\n",
    "        elif (self.end_of_epoch(runner.test_dataloader, batch_idx)\n",
    "              and (not self.ignore_last\n",
    "                   or len(runner.test_dataloader) <= self.interval)):\n",
    "            tag, log_str = runner.log_processor.get_log_after_iter(\n",
    "                runner, batch_idx, 'test')\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "        runner.logger.info(log_str)\n",
    "        runner.logger.info(tag)\n",
    "        runner.visualizer.add_scalars(\n",
    "            tag, step=runner.iter + 1, file_path=self.json_log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8787f4e7",
   "metadata": {},
   "source": [
    "4. Overwrite the original **mmengine** folder with the modified one.\n",
    "5. Update the **MMDetection** folder:\n",
    "    * Locate this file _init_.py and create a new file named **my_hook.py** in this path **mmdetection/mmdet/engine/hooks**.\n",
    "        The use of **my_hook.py** is to prompt the validation loss during training and save it during training using the modified hook we created.\n",
    "        * Copy this entire script to **my_hook.p**y and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594ad819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.hooks import Hook\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "from mmdet.registry import HOOKS\n",
    "\n",
    "from mmengine.hooks.logger_hook import LoggerHook\n",
    "from mmengine.hooks.runtime_info_hook import RuntimeInfoHook\n",
    "\n",
    "\n",
    "@HOOKS.register_module()\n",
    "class MyHook(Hook):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def val_step(self, model, data, optim_wrapper):\n",
    "        with optim_wrapper.optim_context(model):\n",
    "            data = model.data_preprocessor(data, True)\n",
    "            losses = model(**data, mode='loss')\n",
    "        parsed_losses, log_vars = model.parse_losses(losses)  \n",
    "        return log_vars\n",
    "\n",
    "    def after_train_epoch(self, runner) -> None:\n",
    "        model = runner.model\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        optim_wrapper = runner.optim_wrapper\n",
    "        dataloader = runner.test_dataloader\n",
    "        for hook in runner._hooks:\n",
    "            if isinstance(hook, LoggerHook):\n",
    "                logger = hook\n",
    "            elif isinstance(hook, RuntimeInfoHook):\n",
    "                runtimeinfo = hook\n",
    "\n",
    "        for i, data in enumerate(dataloader):\n",
    "            outputs = self.val_step(model, data, optim_wrapper)\n",
    "            # Ensure that the methods exist and are called with the correct arguments\n",
    "            if hasattr(runtimeinfo, 'after_test_iter'):\n",
    "                getattr(runtimeinfo, 'after_test_iter')(runner, None, None, outputs)\n",
    "            if hasattr(logger, 'after_test_iter'):\n",
    "                getattr(logger, 'after_test_iter')(runner, i+1, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b711949",
   "metadata": {},
   "source": [
    "* Update the registration of the custom hook on **_init_.py**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f1bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from .my_hook import MyHook\n",
    "\n",
    "__all__ = [\n",
    "    'MyHook'\n",
    "]   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7525b5",
   "metadata": {},
   "source": [
    "* After updating all the scripts, the next step is to overwrite the mmengine so that the custom hook and loss will run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626e61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import site\n",
    "import os\n",
    "\n",
    "source_path = 'mmengine'\n",
    "overwrite_path = os.path.join(\"{library_path}\", 'mmengine')\n",
    "\n",
    "shutil.rmtree(overwrite_path)\n",
    "shutil.copytree(source_path, overwrite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fac428-5ce4-4363-bd04-f4d7ad9ed1a8",
   "metadata": {},
   "source": [
    "# 4 Preparation of training dataset\n",
    "###  Formats for training datasets\n",
    "\n",
    "### Labeling\n",
    "Labeling Process for Crane Detection in Ship Harbors\n",
    "\n",
    "1.\tLocating Harbor Scenes:\n",
    "    * Search for the harbor location in Google Earth with the extent.\n",
    "\t* Set the eye altitude at 900m and cover the harbor location\n",
    "\t* Export the scene in, including the harbor, as an image file. The guide for exporting the scene from Google Earth is well documented [here](https://glodal.sharepoint.com/sites/GLODAL/_layouts/15/Doc.aspx?sourcedoc=%7Bd4a5c3d5-c2c1-4a88-ac92-93322d905443%7D&action=default&wdLOR=cE0A42B43%2D8477%2D4089%2D9692%2DBBEC9D6AEE90&slrid=ec433ba1-c0cf-3000-840d-d041b7425767&originalPath=aHR0cHM6Ly9nbG9kYWwuc2hhcmVwb2ludC5jb20vOnc6L3MvR0xPREFML0VkWERwZFRCd29oS3JKS1RNaTJRVkVNQk0yUXZQckx3eUg4ZUk2bWNETTJvd2c_cnRpbWU9WVZwczFiQ2gzRWc&CID=924ce444-671d-4815-89a5-1a011eb94f9f&_SRM=0:G:255).\n",
    "2.\tAnnotating Cranes:\n",
    "\t* Open the exported image in the ‘labelme’ tool.\n",
    "\t* Use the ‘Create Polygons’ feature to draw the borders around the cranes in the image.\n",
    "\t* Once the crane annotation is complete, label the polygon as “crane”.\n",
    "\t* Save the annotation, which will be stored in JSON format in the same folder as the image.\n",
    "3.\tConverting Annotations to COCO Format:\n",
    "\t* Use the labelme2coco package to convert the JSON annotations from labelme to the COCO format.\n",
    "\t* Apply the convert method provided by the labelme2coco to perform the conversion.\n",
    "\t\n",
    "This process ensures that the annotations are correctly labeled and converted to a standard coco data format for further analysis in crane detection tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bda2f7",
   "metadata": {},
   "source": [
    "### 4.1 Download Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd748d",
   "metadata": {},
   "source": [
    "To download the demonstration dataset, call 'wget'. The dataset is already processed and ready to be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17159ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -O demo_dataset.zip http://glodal.dynns.com/owncloud/index.php/s/wg9GMNdKAvzUSq0/download #Dataset class instances train:21, test:9   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8829c4",
   "metadata": {},
   "source": [
    "After downloading the dataset, it needs to be unzipped to fully use it for model training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5b688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.unpack_archive(\"./demo_dataset.zip\", \"./demo_dataset\", \"zip\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736a199",
   "metadata": {},
   "source": [
    "**Note**: Data split and conversion are required only if the dataset has not yet been processed (split) and converted into COCO format. In this demo, the dataset has already been processed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aabe300",
   "metadata": {},
   "source": [
    "### Data Split\n",
    "\n",
    "The dataset is split using a randomized approach. The paths of all image files are collected and shuffled to ensure randomness. The dataset is then divided into 70% for the training set and 30% for the validation set, ensuring that the training and validation sets are randomly selected and non-overlapping. Once the images are annotated using Labelme, the data folder will contain .tif image files along with .json files consisting of annotations. The script takes all the images from the ./data folder, splits the data into a 70:30 ratio between train and test sets, and places these sets inside the data_converted_to_coco folder, maintaining the original pairing between image files (.tif) and their corresponding annotation files (.json)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6280698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import math\n",
    "import shutil\n",
    "\n",
    "orig_path = \"./data\"\n",
    "to_path = \"./data_converted_to_coco\"\n",
    "os.makedirs(os.path.join(to_path, \"train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(to_path, \"test\"), exist_ok=True)\n",
    "\n",
    "path_ = glob.glob(os.path.join(orig_path, \"*.tif\"))\n",
    "\n",
    "random.shuffle(path_)\n",
    "split_index = math.ceil(len(path_) * 0.7)\n",
    "\n",
    "list_A = path_[:split_index]\n",
    "list_B = path_[split_index:]\n",
    "\n",
    "for x in list_A:\n",
    "    img_path = x\n",
    "    json_path = x.replace(\".tif\", \".json\")\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(to_path, \"train\", os.path.basename(img_path)))\n",
    "    shutil.copy(json_path, os.path.join(to_path, \"train\", os.path.basename(json_path)))\n",
    "\n",
    "for x in list_B:\n",
    "    img_path = x\n",
    "    json_path = x.replace(\".tif\", \".json\")\n",
    "\n",
    "    shutil.copy(img_path, os.path.join(to_path, \"test\", os.path.basename(img_path)))\n",
    "    shutil.copy(json_path, os.path.join(to_path, \"test\", os.path.basename(json_path)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81ee02",
   "metadata": {},
   "source": [
    "\n",
    "Now we convert the splitted data to COCO format using labelme2coco as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7345816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import labelme2coco\n",
    "\n",
    "labelme2coco.convert('./data_converted_to_coco/train','./data_converted_to_coco/train.json/')\n",
    "labelme2coco.convert('./data_converted_to_coco/test','./data_converted_to_coco/test.json/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e8dd8e",
   "metadata": {},
   "source": [
    "# 5 Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ae110-e04f-4aeb-b75d-e842bd320624",
   "metadata": {},
   "source": [
    "To ensure that all required dependencies are correctly installed and configured before proceeding with further tasks, such as model training or fine-tuning, let's run the following command. This script collects and displays the versions of important packages to verify that the environment is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5deee42b-e839-4c16-b384-c0097f3d4406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.utils import get_git_hash\n",
    "from mmengine.utils.dl_utils import collect_env as collect_base_env\n",
    "\n",
    "import mmdet\n",
    "\n",
    "\n",
    "def collect_env():\n",
    "    \"\"\"Collect the information of the running environments.\"\"\"\n",
    "    env_info = collect_base_env()\n",
    "    env_info['MMDetection'] = f'{mmdet.__version__}+{get_git_hash()[:7]}'\n",
    "    return env_info\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for name, val in collect_env().items():\n",
    "        print(f'{name}: {val}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84299467",
   "metadata": {},
   "source": [
    "## 5.1 Configuration\n",
    "Selecting a backbone is crucial for model training, as it needs to align with the model objectives. MMDetection (version 3.3.0) provides a variety of backbones for users to experiment with. Loading and adjusting the configuration is necessary to meet specific objectives.\n",
    "\n",
    "In this example, Faster R-CNN is used.\n",
    "\n",
    "The settings to configure include:\n",
    "\n",
    "* Path to the dataset\n",
    "* Model architecture\n",
    "* Training hyperparameters, such as learning rate, batch size, number of epochs, etc.\n",
    "* Data augmentation techniques\n",
    "* Configuration of optimizer and learning rate schedule\n",
    "\n",
    "When the config file is edited, use the mmdetection/tools/train.py script provided by MMDetection to start training. This script takes the configuration file as input and handles the training process. During training, the script saves checkpoints at specified intervals, allowing for resuming training or evaluating the model at different stages. The training process can be monitored using command-line outputs and log files. MMDetection also supports TensorBoard for visualizing training metrics like loss and accuracy. Hyperparameters can be adjusted if necessary based on the observed training behavior.\n",
    "\n",
    "\n",
    "To load the configuration correctly within mmdetection/configs, you need to import the configuration module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669727a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.config import Config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5637bf22",
   "metadata": {},
   "source": [
    "* Loading the configuration and outputting its structure is necessary to understand the default configuration of the selected backbone and identify the variables that need to be changed.\n",
    "\n",
    "    To find a config path, e.g., Faster R-CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dfa837",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('mmdetection/configs/faster_rcnn/faster-rcnn_r101_fpn_1x_coco.py')\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7d32bb",
   "metadata": {},
   "source": [
    "### Initializion and Modification\n",
    "After loading and outputting the configuration, there are three main parts that are important to check, update, and double-check:\n",
    "1. Paths, batch size, data root, and more.\n",
    "2. Model parameters such as the number of classes, type of model, and losses. These are essential for fine-tuning but need to be checked before training.\n",
    "3. Train, validation, and test dataloaders, including the pipeline responsible for the datasets used during training, evaluation, and testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a911092",
   "metadata": {},
   "source": [
    "### 5.1.1 Initialization\n",
    "Set the important keys or data in the config dictionary, such as dataset root, output model, and other relevant parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f563388",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.data_root = './demo_dataset/'  # data_root means the path of the dataset that will be used.\n",
    "cfg.dataset_type = 'CocoDataset'   # type of the dataset, indicating its structure. Mostly it's CocoDataset.\n",
    "\n",
    "# Set auto scaling of learning rate parameters:\n",
    "#   - base_batch_size: Batch size used as a base for scaling.\n",
    "#   - enable: Flag to enable auto scaling of learning rate.\n",
    "cfg.auto_scale_lr = dict(base_batch_size=16, enable=True)  # auto_scale will auto set its batch size based on the selected batch size of the dataloaders.\n",
    "cfg.backend_args = None  # Configure backend arguments.\n",
    "\n",
    "cfg.work_dir = './model'  # Output path of the model.\n",
    "\n",
    "\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cdbf92",
   "metadata": {},
   "source": [
    "To ensure that the previous output is now updated and to reduce the risk of errors during training, double-check the keys that were updated. Here’s how you can verify the updates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4e5a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Data root: {cfg.data_root}\")\n",
    "print(f\"Dataset type: {cfg.dataset_type}\")\n",
    "print(f\"Auto scale LR: {cfg.auto_scale_lr}\")\n",
    "print(f\"Backend args: {cfg.backend_args}\")\n",
    "print(f\"Work directory: {cfg.work_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eaeed0",
   "metadata": {},
   "source": [
    "### 5.1.2 Model Setup\n",
    "Choose a pre-defined model architecture from the MMDetection model zoo, such as RPN, Faster R-CNN, Mask R-CNN, RetinaNet, etc. MMDetection provides config files for various pre-trained models. Edit the config file to match the dataset and training settings.\n",
    "\n",
    "Adjusting the model before training is necessary, but not all parameters need to be updated. The key parameter to adjust is the number of classes, which should match the class number of the dataset used. While updating the loss and other keys is not always necessary, basic knowledge of the model structure is required if changes are needed. For this example, Faster R-CNN is used, but there are diverse backbones that can be employed.\n",
    "\n",
    "First, check the model structure to find the keys that should be updated and verify the default values to ensure the model meets your objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc76ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.model)  # Print the model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06381c0d",
   "metadata": {},
   "source": [
    "Here are examples of how to update some values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0a6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update backbone parameters\n",
    "cfg.model['backbone']['depth'] = 101 # change FasterRCNN backbone depth\n",
    "cfg.model['backbone']['init_cfg']['checkpoint'] = 'torchvision://resnet101' # path/to/custom/pretrained.pth; Specify custom pretrained weights\n",
    "cfg.model['backbone']['init_cfg']['type'] = 'Pretrained' # specifies how the model should be initialized or where it should load its initial weights from\n",
    "cfg.model['backbone']['norm_cfg']['requires_grad'] = False  # Disable gradient updates for normalization layer\n",
    "# Update neck parameters\n",
    "cfg.model['neck']['in_channels'] = [256, 512, 1024, 2048]  # Add an additional stage with 4096 input channels\n",
    "cfg.model['neck']['out_channels'] = 256  # Increase output channels to 512\n",
    "\n",
    "# Update ROI Head parameters\n",
    "cfg.model['roi_head']['bbox_head']['loss_bbox']['loss_weight'] = 1.0  # Increase weight for bounding box regression loss\n",
    "cfg.model['roi_head']['bbox_head']['num_classes'] = 2  # Change number of classes to 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1ee3e2",
   "metadata": {},
   "source": [
    "In this example, the number of classes is updated. The model structure is now ready to be used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09c71e7",
   "metadata": {},
   "source": [
    "### 5.1.3 Data Loading\n",
    "Properly loading and configuring the dataset ensures that the model receives the data in the right format and structure, allowing for effective learning and evaluation during the training, validation, and testing phases.\n",
    "\n",
    " 1. **Training Dataloader Setup**: Proper augmentation, batch size, and workers are necessary to ensure the process is error-free and efficient, preventing issues like insufficient memory or incorrect model predictions due to poor augmentation combinations.\n",
    " \n",
    "    First, print the current structure of the training dataloader and its training configuration to check defaults and settings for epochs and validation intervals.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afde77f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.train_dataloader)  # Print current training dataloader structure\n",
    "print(cfg.train_cfg)  # Print training configuration (epochs, validation interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928a5f0",
   "metadata": {},
   "source": [
    "* **Train pipeline** : The augmentations the dataset will undergo before training. Proper augmentation results in better model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c633d5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.train_pipeline = [\n",
    "    dict(backend_args=None, type='LoadImageFromFile'),  # Load image from file\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    #dict(type='CachedMosaic', img_scale=(1024, 1024), pad_val=114.0),  # Cached Mosaic augmentation to reduce memory consumption\n",
    "    dict(type='Resize', keep_ratio=True, scale=(1333, 800)),  # Resize images to the target size\n",
    "    dict(type='RandomFlip', prob=0.5),  # Apply random horizontal flip with 50% probability\n",
    "    #dict(\n",
    "    #    type='CachedMixUp',\n",
    "    #    img_scale=(1024, 1024),  # Cached MixUp augmentation\n",
    "    #    ratio_range=(1.0, 1.0),\n",
    "    #    max_cached_images=20,\n",
    "    #    pad_val=(114, 114, 114)),\n",
    "    dict(type='PackDetInputs'),  # Pack inputs for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5d12cb",
   "metadata": {},
   "source": [
    "Next is the **Train Dataloader** setup, updating batch size for memory efficiency, setting the dataset's training JSON file, and configuring the pipeline to avoid redundant code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c00bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update batch size for the training dataloader\n",
    "cfg.train_dataloader['batch_size'] = 8  # Set batch size to 2\n",
    "cfg.train_dataloader['num_workers'] = 4\n",
    "\n",
    "# Set dataset type\n",
    "cfg.train_dataloader['dataset']['type'] = cfg.dataset_type\n",
    "\n",
    "# Configure training epochs and validation interval\n",
    "cfg.train_cfg['max_epochs'] = 100  # Maximum number of training epochs\n",
    "cfg.train_cfg['val_interval'] = 100  # Interval (in epochs) for validation\n",
    "\n",
    "# Set the annotation file for training dataset\n",
    "cfg.train_dataloader['dataset']['ann_file'] = 'train.json'  # Path to training annotations\n",
    "\n",
    "# Initialize metainfo dictionary if not present\n",
    "cfg.train_dataloader['dataset']['metainfo'] = {}  \n",
    "cfg.train_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "# Set data root directory and image prefix\n",
    "cfg.train_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.train_dataloader['dataset']['data_prefix']['img'] = \"train/images\"  # Path to training images\n",
    "\n",
    "# Assign the train pipeline\n",
    "cfg.train_dataloader['dataset']['pipeline'] = cfg.train_pipeline  # Apply training pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6842bcb7",
   "metadata": {},
   "source": [
    " 2. **Validtion and Test Dataloader Setup**: Validation and Test Dataloader Setup: Both should be configured similarly, as the validation dataloader is used during the model's validation phase, and the test dataloader is used during model evaluation, such as running the test.py script and for generating a confusion matrix.\n",
    "\n",
    "    The reason for minimal augmentation is to match real-world images and scenarios.\n",
    "      * Validation Dataloader: Print its default structure before updating.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b32797",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg.val_dataloader)  # Print current validation dataloader structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320c2f65",
   "metadata": {},
   "source": [
    "Like the training dataloader, the validation dataloader uses the default pipeline with updated dimensions to match the dataset used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa100a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['val_pipeline'] = cfg.test_pipeline\n",
    "cfg.val_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=cfg.backend_args),  # Load image from file\n",
    "    dict(type='Resize', scale=(1024, 1024), keep_ratio=True),  # Resize images to 1024x1024 to match the demo dataset\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')\n",
    "    )  # Pack input metadata for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4131a",
   "metadata": {},
   "source": [
    "Similar to the training dataloader structure, update the JSON and classes, then print to ensure correctness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf35dc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for loading validation data during training\n",
    "cfg.val_dataloader['batch_size'] = 4  # Update the batch size for the validation dataloader\n",
    "cfg.val_dataloader['num_workers'] = 2\n",
    "cfg.val_dataloader['dataset']['type'] = cfg.dataset_type  # Set dataset type\n",
    "\n",
    "cfg.val_dataloader['dataset']['ann_file'] = 'test.json'  # Path to validation annotations\n",
    "\n",
    "cfg.val_dataloader['dataset']['metainfo'] = {}  # Initialize metainfo if not present\n",
    "cfg.val_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "cfg.val_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.val_dataloader['dataset']['data_prefix']['img'] = \"test/images\"  # Path to validation images\n",
    "\n",
    "cfg.val_dataloader['dataset']['pipeline'] = cfg.val_pipeline  # Apply validation pipeline\n",
    "\n",
    "#For the val evaluator\n",
    "cfg.val_evaluator['ann_file'] = os.path.join(cfg.data_root, \"test.json\")\n",
    "\n",
    "print(cfg.val_dataloader)  # Print updated validation dataloader structure\n",
    "print(cfg.val_evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The JSON file is set to test.json instead of val.json because the demo dataset does not contain validation data. However, this can be changed if validation data is available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db68ec2",
   "metadata": {},
   "source": [
    "* **Test Dataloader**: The test dataloader is used during model evaluation, such as running the test.py script and generating a confusion matrix.\n",
    "\n",
    "    First, set up the test pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ea92c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.test_pipeline = [\n",
    "    dict(type='LoadImageFromFile', backend_args=cfg.backend_args),  # Load image from file\n",
    "    dict(type='Resize', scale=(1024, 1024), keep_ratio=True),  # Resize images to 1024x1024 to match the demo dataset\n",
    "    dict(type='LoadAnnotations', with_bbox=True),  # Load annotations with bounding boxes\n",
    "    dict(\n",
    "        type='PackDetInputs',\n",
    "        meta_keys=('img_id', 'img_path', 'ori_shape', 'img_shape', 'scale_factor')\n",
    "    )  # Pack input metadata for detection\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21490a48",
   "metadata": {},
   "source": [
    "Configure the test dataloader with necessary updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a76d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for testing dataset for confusion matrix and model evaluation after training\n",
    "cfg.test_dataloader['batch_size'] = 4  # Update the batch size for the test dataloader\n",
    "cfg.test_dataloader['num_workers'] = 2\n",
    "cfg.test_dataloader['dataset']['type'] = cfg.dataset_type  # Set dataset type\n",
    "\n",
    "cfg.test_dataloader['dataset']['ann_file'] = 'test.json'  # Path to test annotations\n",
    "\n",
    "cfg.test_dataloader['dataset']['metainfo'] = {}  # Initialize metainfo if not present\n",
    "cfg.test_dataloader['dataset']['metainfo']['classes'] = ('gantry_crane', 'standby_gantry_crane')  # Set classes\n",
    "\n",
    "cfg.test_dataloader['dataset']['data_root'] = cfg.data_root  # Root path for dataset\n",
    "cfg.test_dataloader['dataset']['data_prefix']['img'] = \"test/images\"  # Path to test images\n",
    "\n",
    "cfg.test_dataloader['dataset']['pipeline'] = cfg.test_pipeline  # Apply test pipeline\n",
    "\n",
    "cfg.test_evaluator['ann_file'] = os.path.join(cfg.data_root, \"test.json\")  # Set annotation file for the evaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd821a4",
   "metadata": {},
   "source": [
    "### Custom Hook\n",
    "The configuration is already set; now it's time to add the custom hook created earlier.\n",
    "* Define custom_hooks with MyHook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['custom_hooks'] = [dict(type='MyHook')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23d8ee4",
   "metadata": {},
   "source": [
    "Now that the configuration is done, the next step is to save it. MMDetection (3.3.0) config library has a dump feature to save it. This will save to the model output path that was set earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(cfg.work_dir, exist_ok = True)\n",
    "cfg.dump(os.path.join(cfg.work_dir, \"config.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1015e",
   "metadata": {},
   "source": [
    "### Model Training\n",
    "\n",
    "To start model training, run the following script:\n",
    "```\n",
    "python mmdetection/tools/train.py {config path}\n",
    "```  \n",
    "This command will initiate the model training process according to the settings specified in the configuration file, continuing until the specified number of epochs is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6778df48",
   "metadata": {},
   "source": [
    "## 5.4 Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e8be25",
   "metadata": {},
   "source": [
    "### 5.4.1 Detection performance\n",
    "Mean Average Precision is a metric used to measure the performance of a model for tasks such as object detection tasks and information retrieval. It is is a widely used for evaluating the performance of object detection models. It summarizes the precision-recall curve and provides a single number representing the overall performance of the model.\n",
    "\n",
    "* Precision: The ratio of true positive detections to the total number of detections (true positives + false positives).\n",
    "* Recall: The ratio of true positive detections to the total number of ground truth instances (true positives + false negatives).\n",
    "* Average Precision (AP): The area under the precision-recall curve for a single class. It is computed by taking the average of precision values at different recall levels.\n",
    "* Mean Average Precision (mAP): The mean of APs across all classes. It gives an overall performance measure of the detection model across different object categories.\n",
    "\n",
    "In the context of object detection, a model's performance is often reported using mAP at different Intersection over Union (IoU) thresholds (e.g., mAP@0.5, mAP@0.75, mAP@[0.5:0.95])."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333216d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#change the path, folder log differ everytime.\n",
    "with open('./model/20240724_121658/vis_data/20240724_121658.json', 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "current_epoch = None\n",
    "line_number = 0\n",
    "\n",
    "for line in lines:\n",
    "\n",
    "    entry = json.loads(line.strip())\n",
    "    if 'epoch' in entry:\n",
    "        train_losses.append((entry['epoch'], entry['iter'], entry['loss_bbox']))\n",
    "        current_epoch = entry['epoch']\n",
    "    elif 'loss' in entry:\n",
    "        val_losses.append((current_epoch, entry['iter'], entry['loss_bbox']))\n",
    "\n",
    "train_epochs, train_iters, train_loss_values = zip(*train_losses)\n",
    "if val_losses:\n",
    "    val_epochs, val_iters, val_loss_values = zip(*val_losses)\n",
    "    plt.plot(val_epochs, val_loss_values, label='Validation Loss')\n",
    "\n",
    "plt.plot(train_epochs, train_loss_values, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (loss_bbox)')\n",
    "plt.title('Detection Learning Curve for Crane')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653f0c9",
   "metadata": {},
   "source": [
    "### 5.4.2 Classification performance\n",
    "Confusion Matrix is a valuable tool for evaluating the performance of a classification model, including object detection models. It provides a detailed breakdown of the model's predictions, which helps in understanding the strengths and weaknesses of the model. The matrix consists of four categories:\n",
    "\n",
    "* **True Positives (TP)**: The model correctly identifies an object that is present.\n",
    "* **False Positives (FP)**: The model incorrectly identifies an object that is not present (**false alarm**).\n",
    "* **True Negatives (TN)**: The model correctly identifies the absence of an object (**background)**.\n",
    "* **False Negatives (FN)**: The model fails to identify an object that is present.\n",
    "\n",
    "In object detection, the Confusion Matrix may include a background class even if it’s not explicitly defined. This is because the model needs to distinguish between objects and non-objects (**background)**. The matrix helps assess how well the model detects objects and handles background areas.\n",
    "\n",
    "Before creating a Confusion Matrix in MMDetection, a pickle file must be generated using **test.py**, which will test the model and use the dataloader (**test_dataloader**) to create the pickle. This pickle file is then used for generating the Confusion Matrix.\n",
    "\n",
    "Running the following command will generate a .pkl file necessary for the Confusion Matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7537c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#python mmdetection/tools/test.py {config_path} {model_path} --out {pickle_output_path}\n",
    "\n",
    "#example code\n",
    "!python ./mmdetection/tools/test.py \"./output_model/config.py\" \"/content/model/epoch_10.pth\" --out \"./result.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c6d99c",
   "metadata": {},
   "source": [
    "Since MMDetection’s Confusion Matrix has not been updated to handle custom outputs, a custom script is used for model evaluation. This script will produce a plot image that provides a detailed assessment of the model’s performance. The plot will include:\n",
    "\n",
    "* Classes and Background: A visual representation of detected classes and background.\n",
    "* Overall Count of Ground Truth per Class: The total number of true instances for each class.\n",
    "* Count of Incorrect Background Detections: Instances where the model incorrectly identified the background.\n",
    "\n",
    "This approach ensures an accurate evaluation of the model by highlighting both correct detections and areas of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dd4a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.fileio import load\n",
    "from mmdet.utils import replace_cfg_vals\n",
    "from mmengine.registry import init_default_scope\n",
    "from mmdet.registry import DATASETS\n",
    "from mmdet.evaluation import bbox_overlaps\n",
    "import numpy as np\n",
    "\n",
    "def generate_confusionmatrix(config_path, pkl_path, score_thr = 0.3, tp_iou_thr = 0.5):\n",
    "    cfg = Config.fromfile(config_path)\n",
    "    cfg = replace_cfg_vals(cfg)\n",
    "    init_default_scope(cfg.get('default_scope', 'mmdet'))\n",
    "    results = load(pkl_path)\n",
    "    dataset = DATASETS.build(cfg.test_dataloader.dataset)\n",
    "\n",
    "    assert len(results) == len(dataset), \"Please check the dataset\"\n",
    "\n",
    "    num_classes = len(dataset.metainfo['classes'])\n",
    "    confusion_matrix = np.zeros(shape=[num_classes + 1, num_classes + 1])\n",
    "    for idx, per_img_res in enumerate(results):\n",
    "        res_bboxes = per_img_res['pred_instances']\n",
    "        gts = dataset.get_data_info(idx)['instances']\n",
    "    \n",
    "        true_positives = np.zeros(len(gts))\n",
    "        gt_bboxes = []\n",
    "        gt_labels = []\n",
    "        for gt in gts:\n",
    "            gt_bboxes.append(gt['bbox'])\n",
    "            gt_labels.append(gt['bbox_label'])\n",
    "        gt_bboxes = np.array(gt_bboxes)\n",
    "        gt_labels = np.array(gt_labels)\n",
    "    \n",
    "        unique_label = np.unique(res_bboxes['labels'].numpy())\n",
    "    \n",
    "        for det_label in unique_label:\n",
    "            mask = (res_bboxes['labels'] == det_label)\n",
    "            det_bboxes = res_bboxes['bboxes'][mask].numpy()\n",
    "            det_scores = res_bboxes['scores'][mask].numpy()\n",
    "        \n",
    "            ious = bbox_overlaps(det_bboxes[:, :4], gt_bboxes)\n",
    "            for i, score in enumerate(det_scores):\n",
    "                det_match = 0\n",
    "                if score >= score_thr:\n",
    "                    for j, gt_label in enumerate(gt_labels):\n",
    "                        if ious[i, j] >= tp_iou_thr:\n",
    "                            det_match += 1\n",
    "                            if gt_label == det_label:\n",
    "                                true_positives[j] += 1  # TP\n",
    "                            confusion_matrix[gt_label, det_label] += 1\n",
    "                    if det_match == 0:  # BG FP\n",
    "                        confusion_matrix[-1, det_label] += 1\n",
    "        for num_tp, gt_label in zip(true_positives, gt_labels):\n",
    "            if num_tp == 0:  # FN\n",
    "                confusion_matrix[gt_label, -1] += 1\n",
    "\n",
    "    class_labels = ['Gantry Crane', 'Standby Gantry Crane', 'Background']\n",
    "    additional_info = confusion_matrix.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    cax = ax.matshow(confusion_matrix, cmap=plt.cm.Blues)\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    ax.set_xticks(np.arange(confusion_matrix.shape[1]))\n",
    "    ax.set_yticks(np.arange(confusion_matrix.shape[0]))\n",
    "    ax.set_xticklabels(class_labels)\n",
    "    ax.set_yticklabels(class_labels)\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"left\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    for i in range(confusion_matrix.shape[0]):\n",
    "        for j in range(confusion_matrix.shape[1]):\n",
    "            ax.text(j, i, int(confusion_matrix[i, j]), ha='center', va='center', color='black')\n",
    "    \n",
    "    ax.set_xlabel('Predicted labels')\n",
    "    ax.set_ylabel('True labels')\n",
    "    \n",
    "    for idx, info in enumerate(additional_info):\n",
    "        plt.text(0.5, -0.2 - idx * 0.1, f\"{class_labels[idx]}: {info}\", ha='center', va='center', transform=ax.transAxes, fontsize=10)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "config_path = \"./model/config.py\" #configuration path\n",
    "pkl__path = \"./result.pkl\" #pickel outpath\n",
    "\n",
    "generate_confusionmatrix(config_path, pkl__path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51aa7477",
   "metadata": {},
   "source": [
    "Analyzing the Confusion Matrix reveals errors like false positives and false negatives, guiding improvements in the model’s performance, such as tuning parameters, refining training data, or adjusting detection techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce3515",
   "metadata": {},
   "source": [
    "**Note**: An error might occur during processing, as MMDetection version 3 has not yet issued a fix. Manual adjustment to the script may be necessary.\n",
    "\n",
    "For a quick fix in MMDetection version 3, a modification is needed in the runtime_info_hook.py file located in the mmengine/hooks/ folder. Specifically, on line 132, within the after_test_iter function, the following changes should be made:\n",
    "\n",
    "1. Open the runtime_info_hook.py file.\n",
    "\n",
    "2. Locate the after_test_iter function around line 132.\n",
    "\n",
    "3. Add a try and except block to handle the error related to custom validation loss, which occurs when generating the pickle file. The updated function might look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c4c953",
   "metadata": {},
   "outputs": [],
   "source": [
    "def after_test_iter(self,\n",
    "                    runner,\n",
    "                    batch_idx: int,\n",
    "                    data_batch: DATA_BATCH = None,\n",
    "                    outputs: Optional[dict] = None) -> None:\n",
    "\n",
    "    if outputs is not None:\n",
    "        try:\n",
    "            for key, value in outputs.items():\n",
    "                runner.message_hub.update_scalar(f'test/{key}', value)\n",
    "        except Exception as e:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca60ffd1",
   "metadata": {},
   "source": [
    "This adjustment will help bypass the error caused by custom validation loss when generating the pickle file, as this issue arises from using the script specifically for generating the pickle rather than for validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ff773",
   "metadata": {},
   "source": [
    "# 6 Improve model training\n",
    "The models can be improved by performing combination of data augmentation, model architecture adjustments, hyperparameter tuning and some other techniques.\n",
    "\n",
    "## 6.1 Data Augmentation:\n",
    "By performing various augmentations to the training datasets, it can make the model robust and improve generalization. In mmdetection, ‘pipeline’ contains preprocessors of dataset such as augmentation. Some techniques are: Resize, RandomFlip, Normalize, Pad, RandomCrop, ColorTransform, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12d87a3-a773-4105-935b-723b9aa66a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the following parameter to cfg file to use mentioned data augmentation.\n",
    "train_pipeline = [\n",
    "    dict(type='LoadImageFromFile'),\n",
    "    dict(type='LoadAnnotations', with_bbox=True),\n",
    "    dict(type='Resize', img_scale=(1333, 800), keep_ratio=True),\n",
    "    dict(type='RandomFlip', flip_ratio=0.5),\n",
    "    dict(type='RandomCrop', crop_size=(800, 800)),\n",
    "    dict(type='ColorTransform', prob=0.5, level=1),\n",
    "    dict(type='Normalize', mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True),\n",
    "    dict(type='Pad', size_divisor=32),\n",
    "    dict(type='DefaultFormatBundle'),\n",
    "    dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n",
    "]\n",
    "\n",
    "# The choice of selecting augmentation depends upon dataset characteristics, task requirements and domain knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87ef348",
   "metadata": {},
   "source": [
    "## 6.2 Exploring Alternative Models and Datasets for Training\n",
    "Pre-trained detectors from the COCO dataset serve as effective models for initializing on another dataset. Using models available in the Model Zoo can significantly enhance performance. To fine-tune a model for a new dataset, follow these steps:\n",
    "\n",
    "**Inherit Base Configs:** mmdetection supports inheriting configurations from existing setups. Start by inheriting base configurations for model architecture (models/), dataset specifics (datasets/), and runtime settings (default_runtime.py) from mmdetection’s configs directory.\n",
    "\n",
    "**Modify Configurations:** Adjust settings such as model backbone, ROI heads, and dataset paths to suit the characteristics of the new dataset. Modify parameters like num_classes in the ROI head to match the number of classes in the new dataset.\n",
    "\n",
    "**Load Pre-trained Weights:** Initialize the model with weights pretrained on a large-scale dataset (e.g., COCO). This step initializes the model with beneficial learned features for object detection tasks.\n",
    "\n",
    "**Fine-Tuning Process:** Fine-tune the initialized model using the new dataset. Optimize hyperparameters such as learning rate, optimizer type, and batch size to improve performance. Considerations include adjusting learning rates and epochs based on the dataset’s scale and complexity.\n",
    "\n",
    "To use a pre-trained model, specify the path to the pretrained checkpoint in load_from. Ensure the model weights are downloaded before training to minimize download time during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03187c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg.load_from = 'path_to_pretrained_checkpoint.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fe0c1",
   "metadata": {},
   "source": [
    "**Model Architecture Adjustments:** To enhance the object detection capabilities, we can explore alternative models such as YOLOX. YOLOX is known for its efficiency and accuracy in real-time object detection tasks. Here’s how it can be configured and train a YOLOX model using your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfa957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model can be adjusted with following component within cfg.model\n",
    "\n",
    "model = dict(\n",
    "    type='FPN',  # The overall architecture is a Feature Pyramid Network (FPN)\n",
    "    backbone=dict(\n",
    "        type='ResNet',  # The backbone of the FPN is a ResNet model\n",
    "        depth=50,  # The depth of the ResNet backbone (ResNet-50)\n",
    "        init_cfg=dict(\n",
    "            type='Pretrained',  # Indicates that the backbone should be initialized with pretrained weights\n",
    "            checkpoint='torchvision://resnet50'  # Specifies the source of the pretrained weights\n",
    "        )\n",
    "    ),\n",
    "    # ...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750eb43e",
   "metadata": {},
   "source": [
    "Or load the entire configuration provided by MMDetection. To load the configuration, use:\n",
    "\n",
    "```\n",
    "cfg = Config.fromfile('mmdetection/configs/faster_rcnn/faster-rcnn_r101_fpn_1x_coco.py')\n",
    "print(cfg.pretty_text)\n",
    "```\n",
    "\n",
    "Then, follow the guide in section 5. The flow remains the same; the differences lie in augmentation, batch size, or custom hooks like RTMDet. A prior understanding of what has been done is necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2652c785",
   "metadata": {},
   "source": [
    "## 6.3 Hyperparameter tuning:\n",
    " It involves adjusting the settings of a machine learning model to optimizer its performance. These settings, known as hyperparameters, are not learned from the training data but set prior to the training process. Effective tuning of hyperparameters such as learning rate, batch size, optimizer type, and the number of epochs can significantly impact the model’s accuracy and convergence speed. In mmdetection, hyperparameters are configured in the model’s configuration file and can be fine-tuned based on the specific dataset and task requirements to achieve better performance. We have a parameter scheduler, which dynamically adjusts learning rates and other hyperparameter during training to enhance model convergence and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaf13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_hooks = dict(\n",
    "    param_scheduler=dict(\n",
    "        type='ParamSchedulerHook',\n",
    "        scheduler=[\n",
    "            dict(\n",
    "                begin=0,\n",
    "                end=500,\n",
    "                start_factor=0.001,\n",
    "                type='LinearLR'\n",
    "            ),\n",
    "            dict(\n",
    "                begin=0,\n",
    "                by_epoch=True,\n",
    "                end=12,\n",
    "                gamma=0.1,\n",
    "                milestones=[8, 11],\n",
    "                type='MultiStepLR'\n",
    "            )\n",
    "        ]\n",
    "    ),\n",
    "    # ...\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
